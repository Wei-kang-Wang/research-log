---
layout: post
comments: True
title: "基于图片的二维&三维关键点检测"
date: 2024-07-30 01:09:00

---

<!--more-->

{: class="table-of-content"}
* TOC
{:toc}

---


## 2D Keypoint Detection

### Supervised 2D keypoint Detection

#### \[**ECCV 2016**\] [Stacked hourglass networks for human pose estimation](https://arxiv.org/pdf/1603.06937.pdf)

*Alejandro Newell, Kaiyu Yang, Jia Deng*

[CODE](http://www-personal.umich.edu/~alnewell/pose)

![hourglass0]({{ '/assets/images/HOURGLASS-0.PNG' | relative_url }}){: width=200px style="float:center"} 

这篇文章是2D keypoints detection的经典之一（因为它的Hourglass网络结构）。

Hourglass网络的设计受启发于获取各个尺度的信息的需求。尽管局部信息对于识别比如faces或者hands等局部物体来说很重要，但最终的pose estimation需要对整个human body有一个全面的理解。人的orientations，四肢的arrangements，以及相邻关节的关系等等，这些信息都需要同一张图片不同尺度的信息才能良好的获得。Hourglass网络能够将各个尺度的信息综合起来从而输出pixel-wise的预测。
这样的网络必须要有某些机制来有效的处理和合并不同尺度的特征。之前有些工作使用独立的不同的网络来处理不同分辨率的图片输入，再将这些输入综合起来处理。但Hourglass network使用的是一个单一的pipeline，使用skip connections来在各个分辨率下保存空间信息。

Hourglass网络的结构如下：卷积层和max pooling层将输入图片的分辨率降低到一个很低的值。在每个max pooling层，在做max pooling之前，输入分叉为两部分，不进入max pooling的那部分会做更多的卷积操作，留着之后使用。最后在达到了最低分辨率之后，网络开始进行upsampling以及结合之前不同分辨率下的features的操作。为了将两个相邻分辨率的特征结合起来，作者对较低分辨率的feature map使用了最近邻upsampling方法，再加上较高分辨率的那个feature map，从而得到输出。hourglass网络的结构是对称的，之前每用max pooling降低分辨率一次，之后就使用upsampling提升分辨率一次，所以最终的分辨率会和输入图片分辨率一样。在之后，再加上两个卷积大小为$$1 \times 1$$的卷积层，从而得到网络最终的输出。网络的最终输出是一系列的heatmaps，每个heatmap表示的是每个keypoint在图片中每个像素点出现的概率值的大小。

初始输入的图片分辨率为$$256 \times 256$$，为了节约计算成本，作者在将图片输入hourglass模块之前，先通过了一个带有残差链接的$$stride=2$$，$$padding=3$$，卷积核为7的卷积层，和一个max pooling层，从而分辨率变为$$64 \times 64$$，之后在进入上述所说的hourglass模块里。对于hourglass里所有层输出的feature maps，其通道数都是256。

本文提出的最终的网络结果是多个hourglass模块堆叠而成，前一个hourglass模块的输出是后一个hourglass模块的输入。这样的结构使得网络可以重新衡量初始的估计值，改进效果。而且使用这种结构，网络中间某个hourglass模块的输入也可以拿来计算loss，具体做法是，对于中间某个hourglass模块的输出，其经过$$1 \times 1$$卷积得到heatmap输出，从而可以拿来被计算loss，而这个heatmaps再经过$$1 \times 1$$的卷积层回到操作之前的通道数，再和这个hourglass的输出加起来，作为下一个hourglass模块的输入。每个hourglass模块的权重并不共享，而且每个hourglass拿来计算的loss使用的是同一种loss以及同一个ground truth。

本文的evaluation是根据标准的Percentage of Correct Keypoints（PCK）metric来计算的，给定一个normalized的distance，落在ground truth这个distance以内的都算判断正确，而PCK metric计算的是判断正确的keypoints占的比例。本文使用了FLIC和MPII两个数据集来测试效果，对于FLIC数据集，这个distance是利用躯干的大小进行了normalize，而对于MPII数据集，这个distance是根据头的大小进行了normalize。

> 这篇文章并不能解决多人的问题，对于多人的情况，只会考虑最靠近图片中心的那个。如果要看多人的算法，可以去看OpenPose那篇论文。

> 本文的方法不能检测到被遮挡住的keypoints，这仍然是一个需要被解决的问题。


#### \[**CVPR 2017** $$\&$$  **TPAMI 2019**\] [OpenPose: Realtime Multi-Person 2D Pose Estimation Using Part Affinity Fields](https://arxiv.org/pdf/1812.08008.pdf)
*Zhe Cao, Gines Hidalgo, Tomas Simon, Shih-En Wei, and Yaser Sheikh*

[post](https://github.com/CMU-Perceptual-Computing-Lab/openpose)

OpenPose是一个成熟的可以进行multi-person的2D关键点检测的算法（已商业化）

对于单人的pose estimation有很多论文已经做得很好了，但对于多人来说，有以下几个困难的地方：1）首先，我们并不知道图里到底有几个人，而且每个人所在的位置、大小都不清楚；2）其次，人与人之间可能存在干涉，比如说遮挡、关节的旋转等等，很难分清楚到底哪部分属于哪一个人；3）以往的论文里的算法的复杂度都会随着图里人数量的增加而增加，从而很难实现实时。

对于multi-person 2D pose estimation，top-down的方式很常见，也就是先检测图片里有几个人，然后对每个人实行pose estimation，因为单人的pose estimation已经做得很好了，这个算法并不复杂。但它存在着两个很大的问题：首先如果一开始检测人的时候就检测错了或者遗漏了，那之后是没有补救办法的；其次，这样的方法需要对检测出来的每个人都做单人pose estimation，这会使得算法的复杂度和人的数量成正比。所以说，bottom-up的方法也被提了出来，这种方法有能够解决上述两个问题的潜力。但之前的bottom-up方法仍然效率不高，因为它们在最后还是需要利用全局信息来辅助判断，从而要花不少时间。

这篇文章里利用Part Affinity Field实现了实时的multi-person 2D pose estimation。Part Affinity Field是一个2D向量的集合，表示的是四肢的位置和方向信息。利用bottom-up的方式，将detection和association结合起来（模型有两个主要部分，一个是PAF refinement，另一个是body part prediction refinement，而PAF就是association）逐步推进，可以在利用很小的计算资源的情况下达到很好的效果。

![overview]({{ '/assets/images/OPENPOSE-1.PNG' | relative_url }}){: width=400px style="float:center"}
*Fig 1. 算法流程。输入是一张大小为$$w \times h$$的RGB图片，而输出为图中每个人的生理结构上的2D keypoints的位置。首先，一个feedforward网络输出一个集合$$S = (S_1, S_2, ..., S_J)$$有J个confidence maps，每个对应一个身体部位，其中$$S_j \in R^{w \times h}$$，$$j = \lbrace 1,2,...,J \rbrace$$，用来表示各个身体部位位置的2D confidence maps，和一个part affinity fields（也就是2D的向量）的集合$$L= (L_1, L_2, ..., L_C)$$，每个$$L_c \in R^{w \times h \times 2}$$对应一个肢体，$$c \in \lbrace 1,...,C \rbrace$$，用来表示各个身体部位之间的从属程度。最终，confidence maps和PAFs（也就是集合S和集合L）通过greedy inference联系了起来用于输出图中所有人的2D keypoints位置。*

![architecture]({{ '/assets/images/OPENPOSE-3.PNG' | relative_url }}){: width=400px style="float:center"}
*Fig 3. OpenPose里的stages。上图左边部分用来预测PAFs $$L^t$$, 而右侧部分用来预测confidence maps $$S^t$$。每个stage的输出和图片feature连接起来，再作为下一个stage的输入。*

> 注意每个stage的CNN并不共享参数

> OpenPose有两个版本：TPAMI版本和CVPR版本，在CVPR版本里，每个stage都会同时输出PAF和confidence maps，然后结合图片features作为下个stage的输入。而在TPAMI版本里，前$$T_P$$个stages仅仅输出PAF，而后$$T_C$$个stages仅输出confidence map，这样可以大大减少计算量。这样的改进是因为作者通过实验注意到，减少每个stage的输出，并不会太多影响实验结果，而且需要将PAF的stages放在前面。这也符合intuition，因为如果给定一张图片的PAF，很容易猜到每个keypoint在哪里，但如果只是给一系列的keypoints，不仅无法知道单个人的keypoints怎么连接，更不知道keypoints都分别是属于哪个人的。

对于PAF的stage $$t_i$$和confidence map的stage $$t_k$$的loss function分别是：

$$ \mathcal{L}_{P}^{t_i} = \sum_{c=1}^C \sum_p W(p) \lvert L_c^{t_i}(p) - L_c^{\ast}(p) \rvert^2_2, \quad \mathcal{L}_{C}^{t_k} = \sum_{j=1}^J \sum_p W(p) \lvert S_j^{t_k}(p) - S_j^{\ast}(p) \rvert^2_2 $$

其中$$L_c^{\ast}$$是第$$c$$个肢体的PAF的ground truth，$$S_j^{\ast}$$是第$$j$$个keypoint的confidence map的ground truth，上述两个求和都是对所有像素进行的，$$p$$指的就是像素点。$$W(p)$$是图片的mask。

> 在每个stage都使用loss function，用来解决梯度消失的问题，因为每个stage结尾都有loss function，对梯度的值进行了补充。

网络整体的loss function就是$$ \mathcal{L} = \sum_{t=1}^{T_P} \mathcal{L}_{P}^{t} + \sum_{t=T_P + 1}^{T_P + T_C} \mathcal{L}_{C}^t $$

Confidence maps GT，$$S_j^{\ast}$$的计算方式是，先对于图片里的每个人$$k$$生成confidence map，$$S_{j,k}^{\ast}$$。$$x_{j,k} \in \mathbb{R}^2$$表示人$$k$$的keypoint$$j$$的ground truth。从而$$ S_{j,k}^{\ast}(p) = exp(-\lvert p-x_{j,k} \rvert^2_2 / \sigma^2) $$，其中$$\sigma$$控制峰的大小。从而整个图片（可能包含多个人）的ground truth就是：$$ S_j^{\ast}(p) = \max\limits_{k} S_{j,k}^{\ast}(p) $$

Part Affinity Fieds (PAFs) GT的计算方式较为复杂一点。每一个PAF都是一个2D的vector field。对于每个肢体的PAF的每个像素位置的值，其都是一个2D的向量，包含了位置和这个肢体一个keypoint指向另一个keypoint的方向。预先定义好的body part keypoints对决定了哪些有哪些keypoints对构成肢体。

对于某张图片，其可能含有多个人，假设$$x_{j_1, k}$$和$$x_{j_2, k}$$是人$$k$$的两个keypoints，这keypoints对（有序）组成了肢体$$c$$。对于肢体$$c$$上的一点$$p$$，$$L_{c,k}^{\ast}(p)$$是一个单位向量，从$$j_1$$指向$$j_2$$，对于其它的点，$$L_{c,k}^{\ast}(p)$$的值都是0。但肢体不仅是一条直线，而是一个具有一定宽度的长方形。具体来说：$$v = (x_{j_2, k} - x_{j_1, l}) / \lvert x_{j_2, k} - x_{j_1, l} \rvert$$是从$$x_{j_1, k}$$指向$$x_{j_2, k}$$的单位向量，$$v_{\bot}$$是垂直于$$v$$的一个单位向量，而对于任意的像素点$$p$$，如果其满足$$ 0 \leq v (p - x_{j_1,k}) \leq \lVert x_{j_2, k} - x_{j_1, k} \rVert$$ 和 $$\lvert v_{\perp} (p - x_{j_1,k}) \rvert \leq \sigma_l$$，那么就认为$$p$$在肢体$$c$$上，从而$$L_{c,k}^{\ast}(p) = v$$，别的像素点位置的值就是0（长度为2的0向量）。

而整个图片的PAF的ground truth是对于所有人取了均值：$$ L_c^{\ast}(p) = 1/n_c(p) \Sigma_k L_{c,k}^{\ast}(p) $$，其中$$n_c(p)$$统计的是在点$$p$$处非零向量的PAF的个数，也就是对应着有相交的肢体的情况。

有了上述confidence map和PAF的GT的定义，就可以开始网络的训练了。

而在测试过程中，对于输入的图片，对于每个keypoint，能得到一个预测的confidence map，使用non-maximum suppresion（非极大抑制）可以得到该keypoints可能出现的多个位置，这些多个位置可能对应着图片里多个人的该keypoint，但也可能是假阳性。为了进一步筛选，就需要使用预测的PAF。对于每张图片，我们得到了$$C$$个PAF，对于每个PAF，我们计算连接该PAF对应的两个keypoints之间的线段上PAF的积分来作为score，用来衡量这两个keypoints是否构成了一个肢体。

> 如果这两个keypoints按照之前的non-maximum suppression的计算，都有好几个预测值，那么对于每对都需要计算该积分

这个积分具体是这样算的，对于两个检测到的keypoints，$$d_i$$和$$d_j$$：

$$ E_{d_i, d_j} = \int_{u=0}^{u=1} L_c(p(u)) (d_j - d_i)/ \lVert d_j - d_i \rVert du, \quad p(u) = (1-u) d_i + u d_j$$

> 在具体操作的时候，上述积分使用离散点的和来近似

从而我们将从一系列所有keypoints的candidates里挑选最终的keypoint set的问题描述为了一个最大化所有的PAF对应的score的和的优化问题，这个优化问题的约束条件是，每个PAF必须包含且仅包含该PAF对应的两个keypoints的分别各一个candidate。然而这个优化问题是个NP-hard的问题。这篇文章使用一种greedy relaxation的方法，持续性的产生高质量的匹配。

> 文章猜测这种方法有效的原因是上述计算的积分值（也就是每个肢体的score）潜在的含有global信息，因为PAF的框架具有较大的感受野

具体来说，首先，我们获得整张图片所有keypoints的所有candidates的集合，$$D_J = \lbrace d_{j}^{m}: j = 1, ..., J, m = 1, ..., N_j \rbrace$$，$$N_j$$是keypoint $$j$$的candidates的数量，而$$d_j^m \in R^2$$是keypoint $$j$$的第$$m$$个candidate的position。定义$$z_{i,j}^{m,n} \in \{0, 1\}$$来表示两个keypoint $$i,j$$的两个候选$$d_{i}^m$$和$$d_{j}^n$$是否构成肢体。肢体的候选集合为$$Z = \{z_{i, j}^{m,n}: i, j \in \{1, ..., J\}, m \in \{1, ..., N_i \}, n \in \{1, ..., N_j \}$$。

严格来描述这个优化问题，对于一对keypoint对$$i$$和$$j$$（比如说neck和right-hip），以及它们组成的肢体$$c$$，目标是：

$$ \max\limits_{Z_c} E_c = \max\limits_{Z_c} \sum_{m \in D_i} \sum_{n \in D_j} E_{m,n} z_{i, j}^{m,n}, \quad s.t., \forall m \in D_i, \sum_{n \in D_j} z_{i, j}^{m, n} \leq 1, \quad \text{and} \quad \forall n \in D_j, \sum_{m \in D_i} z_{i, j}^{m,n} \leq 1$$

> 上述定义的constraint，使得我们所优化到的结果里不会有两个$$c$$肢体公用同一个keypoint。而对于该优化目标$$E_c$$，我们可以用Hungarian算法来获取上述优化的结果。

但问题的目标$$Z$$是所有的肢体的目标$$Z_c$$的和，计算$$Z$$的最优值是一个$$K-Matching$$问题（$$K$$是肢体的数量），这个问题是个NP-hard的。对于该问题，有很多relaxations的算法存在。在这篇论文中，因为按照对人体的肢体的定义，所有的肢体$$c$$以及所有的keypoints构成的集合是一个spanning tree，作者将上述$$K-Matching$$问题分解为一系列二分匹配的子问题并且独立的解决这些问题，也就是将$$Z$$的优化分解成了独立对每个$$Z_c$$进行优化，即：

$$ \max\limits_{Z} E = \sum_{c=1}^C \max\limits_{Z_c} E_c, \quad s.t., \forall c, \quad \forall m \in D_i, \sum_{n \in D_j} z_{i, j}^{m, n} \leq 1, \quad \text{and} \quad \forall n \in D_j, \sum_{m \in D_i} z_{i, j}^{m,n} \leq 1 $$

> 第二个relaxation之所以可行，直觉上来说，spanning tree里相邻的两个node之间的关系是由预测PAF的网络学习到的，而非相邻的两个node之间的关系是由预测Keypoint的网络学习到的。

从而我们将这个优化问题分解为独立的每个pair的优化问题，而这个在之前所述，可以用Hungarian算法解决。我们再将有共同keypoint的肢体联合起来，这样其就表示出了一个完整的人的pose，或者说骨架。


### Unsupervised 2D Keypoint Detection

#### \[**ICCV 2017**\] [Unsupervised learning of object landmarks by factorized spatial embeddings](https://openaccess.thecvf.com/content_ICCV_2017/papers/Thewlis_Unsupervised_Learning_of_ICCV_2017_paper.pdf)

[CODE](https://github.com/alldbi/Factorized-Spatial-Embeddings)

这篇文章的方法基于factorizing image deformations，这种deformation可能是由viewpoint变化引起的，或者是因为物体本身的形变引起的。作者提出的方法基于的假设是deformation前后的图片的keypoints是consistent的。

> 实际上，通过对图片加上人为构造的deformation，将原始图片上所检测到的keypoint加上deformation，以及加上deformation之后的图片所检测到的keypoint，的差异作为约束条件，是现如今非常常见的一种约束keypoint detection的辅助loss，在FewShot3DKP，StableKeypoint里都有这样的loss

$$S \subset \mathbb R^3$$表示一个物体的3维surface，比如说一只鸟，$$\pmb x: \Lambda \rightarrow \{0,1,\cdots,255 \}^3$$表示的是这个物体的一张RGB图片，其中$$\Lambda$$是image grid。$$S$$是3维物体的形状，其和$$\pmb x$$是无关的。作者考虑的是学习一个函数$$q = \Phi_S(p; \pmb x)$$将$$S$$上的点$$p \in S$$映射到对应的图片$$\pmb x$$上的某个点$$q \in \Lambda$$上去。作者提出了一个利用viewpoint factorization来自动学习$$\Phi_S$$的方法。为了实现这个目标，我们考虑从另一个角度来看这个物体从而获得的另一张图片$$\pmb{x^{'}}$$。利用$$g$$来表示由于视角变化导致的图片歪曲，也就是$$\pmb{x^{'}} \approx \pmb x \circ g$$。利用$$\Phi_S$$，我们可以将$$g$$$分解为：

$$g = \Phi_S(\dot; \pmb{x^{'}}) \circ \Phi_S(\dot; \pmb x)^{-1}$$

等价于：$$\forall q \in S: \Phi_S(p; \pmb x \circ g) = g(\Phi_S(p; \pmb x))$$，这个约束条件表明$$S$$上所检测到的点$$p$$需要随着视角的变化仍然保持一致。

> 上述的$$g$$可以用来描述由于视角、物体本身的deformation以及category-specific image collection里不同个体的shape variance

为了学习这个函数$$\Phi_S$$，作者将这个函数利用一个深度神经网络来表示，并采用一种Siamese的结构，输入是$$(\pmb x, \pmb{x^{'}}, g)$$

> 如果我们任意给定一个物体的两个视角的图片，那么$$g$$一般是不知道的，所以说与其再利用某种方法来估计$$g$$，不如直接随机的构造出$$g$$，再利用$$g$$从$$\pmb x$$上构造出$$\pmb{x^{'}}$$。

$$\Phi_S$$是由神经网络所表示的，可以使用现有的任何一种用于keypoint detection的框架来实现它（比如HRnet，Hourglass等）。网络的输入是图片$$x$$，输出是检测到的keypoints，主要的loss如下：

$$\mathcal L_{align} = \frac{1}{K} \sum_{r=1}^K \lVert \Phi(\pmb x_r \circ g) - g(\Phi(\pmb x_r)) \rVert^2$$

上述的loss会使得网络学习到的keypoints都是consistent的，但并没有阻止网络所学习到的都是同一个点，也就是这些点全都聚集到了一起。为了避免这个现象，作者加了一个diversity loss来使得这些probability maps对应到图片的不同区域。最直接的方式就是直接对两个keypoints的heatmap的相互重合的区域增加惩罚：

$$\mathcal L_{div}(\pmb x) = \frac{1}{K^2} \sum_{r=1} \sum_{r^{'}=1} \sum_u p(u \vert \pmb x, r) p(u \vert \pmb x, r^{'})$$

上述公式的一个缺点就是计算量是quadratic的。一个简便的方法是：

$$\mathcal L_{div}^{'} (\pmb x) = \sum_u (\sum_{r=1}^K p(u \vert \pmb x, r) - \max\limits_{r=1, \cdots, K} p(u \vert \pmb x, r))$$

具体实现的时候，对于一张图片$$\pmb x$$和用thin-plate splines（薄板样条插值）TPS实现的transformation $$g_1, g_2$$，网络的输入图片对为：$$g_1 \circ \pmb x$$和$$g_2 \circ (g_1 \circ \pmb x)$$。

> 该文章最大的问题在于，所描述的keypoints在经过deformation之后仍要保持一致，这只能对于很小的deformation来说，因为大的deformation会直接导致keypoints可能不再可见。而该文章最大的贡献在于，如同前面所说，其提出的keypoints在图片经过小的deformation之后仍需要保持consistency这个性质，仍然被很多unsupervised的keypoint甚至structure检测，或者part segmentation论文当作辅助loss来使用。


#### \[**CVPR 2018**\] [Unsupervised Discovery of Object Landmarks as Structural Representataions](https://openaccess.thecvf.com/content_cvpr_2018/papers/Zhang_Unsupervised_Discovery_of_CVPR_2018_paper.pdf)

*Yuting Zhang, Yijie Guo, Yinxin Jin, Yijun Luo, Zhiyuan He, Honglak Lee*

[POST](https://www.ytzhang.net/projects/lmdis-rep/)

本文提出了一个autoencoder的框架来将检测到的landmarks显式的表示为structural representations。encoder输出的是landmarks，其加了限制从而能够输出有效的landmarks信息；而decoder模块以这些landmarks作为输入重构输入图片，从而形成一种end-to-end的结构来学习这些landmarks。

[Unsupervised learning of object landmarks by factorized spatial embeddings](https://openaccess.thecvf.com/content_ICCV_2017/papers/Thewlis_Unsupervised_Learning_of_ICCV_2017_paper.pdf)提出了一个无监督学习的方式在突破可能有transformations的情况下来检测稳定的landmarks表示物体的局部语义信息。然而，这个方法并没有显式的鼓励这些landmarks出现在那些可以用作image modeling的位置上。

![usr1]({{ '/assets/images/USR-1.PNG' | relative_url }}){: width=400px style="float:center"}

作者使用一个深度神经网络来将输入图片$$\pmb I$$转换为输出是$$K+1$$个通道的detection confidence map $$\pmb D \in \left[0,1\right]^{W \times H \times (K+1)}$$。这个confidence map会检测$$K$$个landmarks，而第$$K+1$$个通道表示的是背景。$$\pmb D$$的分辨率$$W \times H$$可以和输入图片的分辨率一样，或者更小，但是它们需要是同比例的。

作者提出一个轻量化的hourglass网络，来从输入图片中获取原始的detection score map $$\pmb R$$：

$$\pmb R = hourglass_l (\pmb I; \theta_l) \in \mathbb R^{W \times H \times (K+1)}$$

其中$$\theta_l$$表示的是网络参数。hourglass网络可以让detectors兼顾检测局部特征和利用全局信息。在获得了$$\pmb R$$之后，再在$$\pmb R$$的每个位置都做一次softmax，也就是每个位置沿着所有的通道数做一次softmax（包括表示背景的score map），从而所得到的$$\pmb D$$的每个位置，沿着所有通道的值加起来就是1，$$\pmb D$$里的每个值都在0到1之间：

$$\pmb D_k(u,v) = \frac{exp(\pmb R_k(u,v))}{\sum_{k^{'}=1}^{K+1} exp(\pmb R_{k^{'}}(u,v))}$$

其中$$\pmb D_k$$是$$\pmb D$$的第$$k$$个通道。

将$$\pmb D_k$$看作一个加权的map，作者使用加权的均值坐标作为第$$k$$个landmark的坐标，也就是：

$$(x_k, y_k) = \frac{1}{\zeta_k} \sum_{v=1}^H \sum_{u=1}^W (u,v) \cdot \pmb D_k(u,v)$$

其中$$\zeta_k = \sum_{v=1}^H \sum_{u=1}^W \cdot \pmb D_k(u,v)$$是空间归一化因子。将landmark和landmark detector简记为：

$$\pmb l = \left[x_1, y_1, \cdots, x_K, y_K \right]^T = \text{landmark}(\pmb I; \theta_l)$$

流程图上半部分（蓝色）的左半部分表示的就是landmark detector。

$$\pmb l$$里的元素应该是所检测到的landmark coordinates，但是到目前为止，并没有限制条件使得它们是landmarks，到现在为止它们只是任意的latent representations。因此，作者提出了下列soft constraints作为正则项来迫使这些所检测到的representations具有landmarks的特性。

* Concentration constraint

作为一个单一location的detection confidence map，$$\pmb D_k$$的值需要集中于一个局部的区域内。$$\pmb D_k / \zeta_k$$可以被认为是一个2维的概率分布，从而可以计算出其沿着$$x$$轴和$$y$$轴的方差，$$\sigma_{det, u}^2, \sigma_{det, v}^2$$。作者定义了如下的constraint loss来鼓励这两个方差都很小：

$$L_{conc} = 2\pi e (\sigma_{det, u}^2 + \sigma_{det, v}^2)^2$$

上述的表示的是以$$(x_k,y_k)$$为中心，以$$(\sigma_{det,u}^2 + \sigma_{det, v}^2)/2 \cdot \mathbb I$$为协方差矩阵的二维高斯分布的熵的exponential。这个高斯分布是$$\pmb D_k / \zeta_k$$的一个近似，$$L_{conc}$$越小，分布就越集中于中心点的位置，越符合要求。

该高斯分布记为：$$\overline{ \pmb{D}_k}(u,v) = (1 / WH) \mathcal N((u,v); (x_k, y_k), \sigma_{det} \mathbb I)$$

* Separation constraint

理想情况下，autoencoder的目标函数可以自动使得$$K$$个landmarks分布在不同的区域内从而很好地完成decoder里的reconstruction任务。然而，由于随机的初始化，landmarks可能互相靠得很近。这会导致优化过程陷入局部最优点。为了解决这个问题，作者提出了一个显式的loss来空间分隔这些landmarks：

$$L_{sep} = \sum_{k \neq k^{'}, k=1}^{K} exp(\frac{-\lVert (x_{k^{'}}, y_{k^{'}}) - (x_k, y_k) \rVert_2^2}{2 \sigma_{sep}^2})$$

* Equivariance constraint

图片的某一个特定的landmark应该位于一个具有明显局部特征的地方（这个局部特征也应该有明确的语义信息）。这需要landmarks对于image transformations来说具有equivariance的特性。更具体来说，如果相对应的视觉信息仍然存在于transformed之后的图片中的话，一个landmark应该要跟着transformation变化而变化（比如说camera或者object的移动）。$$g$$用来表示这种transformation，从而原image $$\pmb I$$和transformed之后的image $$\pmb I^{'}$$就有着如下的对应关系：$$\pmb I^{'}(u,v) = \pmb I(g(u,v))$$，也就是说，$$g(x_k^{'}, y_k^{'}) = (x_k ,y_k)$$，从而引入如下的限制：

$$L_{eqv} = \sum_{k=1}^K \lvert g(x_k^{'}, y_k^{'}) - (x_k, y_k) \rvert_2^2 $$

如果$$g$$是已知的话，那么这个loss就是定义好了的。和[Unsupervised learning of object landmarks by factorized spatial embeddings](https://openaccess.thecvf.com/content_ICCV_2017/papers/Thewlis_Unsupervised_Learning_of_ICCV_2017_paper.pdf)一样，作者使用一个thin plate spline（TPS）来使用随机参数模拟$$g$$。作者使用随机的translation，rotation以及scaling来为TPS确定global affine component；之后再perturb一系列control points来确定TPS的local component。除此之外，如果训练集合是以视频的形式出现的，可以将dense motion flow用作$$g$$，而下一帧就是$$\pmb I^{'}$$。

除了keypoint detector之外，这篇文章的另一个贡献是对于每个keypoint，还align了一个local feature descriptor。

对于简单的图片，比如说MINIST，landmarks就足够描述物体的形状了。但对于大多数实际的图片来说，landmarks并不足以表示所有的视觉内容，所以就需要额外的latent representations来encode补充信息。而一方面，我们不能引入过多的全局信息，因为这样会导致模型不容易学习到landmark的局部信息（因为这样的全局信息就足够decoder完成reconstruction任务了）；而另一方面，我们也需要一定的全局信息来帮助landmarks的定位。为了解决这个trade-off，作者给每个landmark都计算了一个low-dimensional的local descriptor。

作者使用另一个hourglass网络来获取一个feature map $$\pmb F$$，其和detection confidence map $$\pmb D$$的尺寸是一样的：

$$\pmb F = \text{hourglass}_f(\pmb I; \theta_f) \in \mathbb R^{W \times H \times S}$$

对于每个landmark来说，作者使用一个average pooling来获取这个landmark的local feature descriptor，其中这个average pooling的权重是由一个中心在这个landmark点的soft mask构成的。具体来说，我们将之前所说的中心点位于$$\left[ x_k, y_k \right]$$的二维高斯分布$$\overline{ \pmb{ D}_k}$$作为这个soft mask。一个可学习的linear operator（实际上就是几层MLP）被加在每个feature上从而为每个landmark学习到一个独立的feature descriptor（注意到每个landmark都有一个自己的linear operator，不是共享的）。

> 作者认为之前所学习到的feature $$\pmb F$$使得所有keypoint的features都公用一个空间，而此处对于每个landmark使用不同的linear projection将其映射到自己独立的空间里。每个landmark独有的这个linear operator使得每个landmark descriptor可以编码独有的信息

具体来说，第$$k$$个landmark的latent descriptor就是：

$$\pmb f_k = \pmb W_k \sum_{v=1}^H \sum_{u=1}^W (\overline{ \pmb{ D}_k}(u,v) \cdot \pmb F(u,v)) \in \mathbb R^{C}, \quad \text{with} \quad C < S$$

我们还需要获取一个背景的descriptor。但使用上述方法利用一个高斯分布来近似背景的confidence map是不合理的，所以直接令$$\overline{ \pmb{ D}_{K+1}} = \pmb D_{K+1} / \zeta_{K+1}$$。

将所有的landmarks的descriptors和背景的descriptor放在一起，我们就有了$$\pmb f = \left[ \pmb f_1, \pmb f_2, \cdots, \pmb f_{K+1} \right] \in \mathbb R^{C \times (K+1)}$$。流程图的下半部分的左边部分表示了获取这个landmark descriptors $$\pmb f$$的过程。

最后介绍detector的设计。

作者先从所检测到的landmark coordinates恢复detection confidence map $$\tilde{ \pmb{ D}} \in \mathbb R^{W \times H \times (K+1)}$$。具体来说，作者使用一个二维高斯分布来表示每个landmark的confidence map，其中心点是每个landmark的coordinates，其协方差矩阵是$$\sigma_{det}^2 \mathbb I$$，其中$$\sigma_{dec}$$不是之前计算出来的，是一个超参数。

$$\tilde{ \pmb{R}_k}(u,v) = \mathcal N((u,v); (x_k, y_k), \sigma_{dec}^2 \mathbb I), k=1,2,\cdots,K, \tilde{ \pmb{ R_{K+1}}} = \pmb 1$$

> $$\tilde{ \pmb{R}_k}(u,v)$$和$$\overline{ \pmb{ D}_k}(u,v)$$的区别就在于前者的协方差是固定的，后者的协方差是由encoder的heatmap算出来的。而且decoder里对background的描述也和descriptor里的不一样。

对于计算好的$$\tilde{ \pmb{R}_k}$$中，对于每个像素点位置，沿着所有的通道做一次归一化，从而得到detection score map $$\tilde{ \pmb{D}}$$：

$$\tilde{ \pmb{D}_k}(u,v) = \tilde{ \pmb{R}_k}(u,v) / \sum_{k=1}^{K+1} \tilde{ \pmb{ R}_k}(u,v)$$

流程图的上半部分的右半部分描述了这个过程。

对于之前feature descriptor network得到的每个landmark的descriptor $$f_k$$（也包括背景的），再利用一个landmark-specific linear operator $$\tilde W_k$$将每个descriptor $$f_k$$再次进行线性映射（其后再接上一个activation function，文中使用的是LeakyReLU），最后对于这些映射后的features，再次使用$$\tilde{ \pmb{ D}_k}(u,v)$$将它们unpooling成一个feature descriptor $$\tilde {\pmb {F}}$$：

$$\tilde {\pmb {F}}(u,v) = \sum_{k=1}^{K+1} \tilde{ \pmb{ D}_k}(u,v) \cdot \tau(\tilde{ \pmb{ W}_k} \pmb f_k) \in \mathbb R^{W \times H \times S}$$

其中$$\tau$$就是activation function。这个过程由流程图下半部分的右半部分所描述。

每个landmark对应的高斯分布的方差$$\sigma_{dec}^2$$决定其周围的邻居可以给这个landmark贡献多少信息。在训练一开始的时候，需要比较大的$$\sigma_{dec}^2$$来使得训练可行，也就是说每个landmark要依靠的邻居点还很多，而随着训练的进行，需要更准确的定位，也就是需要比较小的方差。对于这样两个相互矛盾的需求，作者使用不同的$$\sigma_{dec}$$来获取多个版本的$$\tilde{ \pmb{D}}$$和对应的$$\tilde{\pmb{F}}$$：$$(\tilde{ \pmb{D}_1}, \tilde{\pmb{ F}_1}), (\tilde{ \pmb{ D}_2}, \tilde{ \pmb{ F}_2}), \cdots, (\tilde{ \pmb{ D}_M}, \tilde{ \pmb{ D}_M})$$。

最后，将上述这些$$(\tilde{ \pmb{ D}_1}, \tilde{ \pmb{ F}_1}), (\tilde{ \pmb{ D}_3}, \tilde{ \pmb{ F}_2}), \cdots, (\tilde{ \pmb{ D}_M}, \tilde{ \pmb{ D}_M})$$沿着通道这个维度连起来，再输入一个hourglass网络里，最终获取重建的图片：

$$\tilde{ \pmb{ I}} = \text{hourglass}_d (\left[\tilde{ \pmb{ D}_1}, \tilde{ \pmb{ F}_1}), (\tilde{ \pmb{ D}_2}, \tilde{ \pmb{ F}_2}), \cdots, (\tilde{ \pmb{ D}_M}, \tilde{ \pmb{ D}_M} \right]; \theta_d)$$

流程图里的最右边的灰色线表示了这个过程。

图片reconstruction loss驱动了整个training的进行。整个网络的loss $$L_{AE}$$定义为：

$$L_{AE} = \lambda_{recon} L_{recon} + \lambda_{conc} L_{conc} + \lambda_{sep} L_{sep} + \lambda_{eqv} L_{eqv}$$


#### \[**NeurIPS 2018**\] [Unsupervised Learning of Object Landmarks through Conditional Image Generation](https://proceedings.neurips.cc/paper/2018/hash/1f36c15d6a3d18d52e8d493bc8187cb9-Abstract.html)

[PAGE](https://www.robots.ox.ac.uk/~vgg/research/unsupervised_landmarks/)

该方法的思想是，对于一对图片输入，$$\pmb{x}, \pmb{x}^{'}$$（其可能是一个视频的两帧，或者是synthetic object的deform前后的两张图片），可能具有appearance、deformation以及viewpoint的变化（但是是同一个object的两张图片），利用从$$\pmb{x}^{'}$$获取的structure信息（显式的由keypoints表示）以及从$$\pmb{x}$$获取的appearance信息，如果一个decoder能够reconstruct $$\pmb{x}^{'}$$，那么就说明我们获取的structure信息是足够好的。

流程图如下：

![usr1]({{ '/assets/images/conditiongenekp_1.png' | relative_url }}){: width=400px style="float:center"}

对于一对输入，$$\pmb{x}, \pmb{x}^{'}$$，使用$$\Phi(\pmb{x}^{'})$$获得$$K$$个heatmap，得到$$K$$个最大值点的位置，构造$$K$$个Gaussians，沿着channel维度连接起来，得到$$\pmb{y}^{'} \in \mathbb{R}_{+}^{H_o \times W_o \times K}$$，再和图片$$\pmb{x}$$的feature沿着channel维度连接起来，输入给decoder $$\Psi$$得到reconstruction的结果：$$\overline{\pmb{x}}^{'} = \Psi(\pmb{x}, \pmb{y}^{'})$$，而损失函数即为$$\overline{\pmb{x}}^{'}$$和$$\pmb{x}^{'}$$之间的MSE。

> 本方法的思想其实和AutoLink的也是有相似之处的，都是希望keypoints有足够的structure信息能够让decoder可以reconstruct回原图片，但AutoLink引入了edge map来表示keypoints的structure信息，更加fancy


#### \[**CVPR 2019**\] [D2-Net: A Trainable CNN for Joint Description and Detection of Local Features](https://openaccess.thecvf.com/content_CVPR_2019/papers/Dusmanu_D2-Net_A_Trainable_CNN_for_Joint_Description_and_Detection_of_CVPR_2019_paper.pdf)

*Mihai Dusmanu, Ignacio Rocco, Tomas Pajdla, Marc Pollefeys, Josef Sivic, Akihiko Torri, Torsten Sattler*


在images之间建立像素点的对应关系是一个很基础的CV问题，其可以被应用在3D CV，视频压缩，跟踪，定位等等任务里。稀疏的局部特征是预估对应关系的一个重要方法。这些方法基于一种detect-then-describe的模式，先利用一个feature detector来找到一批keypoints，然后根据这些keypoints和其周围的点构建一些image patches，再利用descriptor对这些image patches给出feature description，作为这些keypoints的features。稀疏的局部特征方法有以下几个优势：1）对应关系可以通过nearest neighbor search和欧式距离很快的被找到。2）而且稀疏的特征所消耗的内存小，使得方法能够在大规模问题上应用。3）而且基于这种方法的keypoint detector一般都会考虑low-level的图像信息，比如说corners。从而局部特征可以被精确的在图像中定位，这对于很多后续任务比如说3D reconstruction来说是很重要的。

稀疏局部特征方法在很多图像条件下都得到了很好的使用。但是他们在极端的图像appearance变化的情况下就不好使了，比如说白天和夜里，或者季节变化，或者很弱纹理的场景。研究表明稀疏局部特征方法在这些情况下不好使的原因是keypoint detector检测到的keypoint的repeatability很差：因为按照上述描述的detector-then-describe流程，local descriptor会考虑keypoint和周围点的一块较大范围内的信息，从而潜在的会encode更high-level的结构，但keypoint detector只会考虑一个keypoint点的信息，范围太小。从而，这些keypoints的detections在appearance变化很大的时候就不稳定了。这是因为low-level信息会被图像里的low-level statistics影响更大，比如说像素点的intensity。

在这篇文章里，作者旨在同时做好这两件事情，也就是，找到在复杂条件下也能有很好效果的稀疏的特征集合，而且设计高效的匹配算法。为了达到这个目标，作者提出了一个describe-and-detect的方法来进行稀疏局部特征detection和description：和之前先利用low-level信息进行feature detection的方法不同，这个方法将detection stage放在了后面。该方法首先利用CNN计算feature maps，再利用这些feature maps来计算descriptors（在feature maps的每个像素位置，所有通道构成的向量为这个像素点的feature description）和检测keypoints（在feature maps上找到局部最大值）。通过这样操作，feature detector和feature descriptor就紧密相连了。由detector检测到的keypoints就会是那些匹配效果很好的descriptors了。同时，使用CNN深层输出的feature maps使得我们能够基于high-level的信息来计算feature detection和feature description。实验表明这种方法和dense方法相比所需要的内存要小得多，同时其也比之前的detect-then-describe方法对于复杂条件下的images的匹配效果要好得多，和dense方法效果差不多甚至更好。

和传统的使用了两个stages的pipeline的detect-then-describe方法不一样，这篇文章提出使用dense feature extraction来获得一个同时是detector和descriptor的representation。因为detector和descriptor使用了同样的representation，文章将这个方法叫做D2。方法流程如fig 1所示。

![performance]({{ '/assets/images/D2_1.png' | relative_url }}){: width=400px style="float:center"}
*Fig 1. detect-and-describe (D2) network。用一个feature extraction CNN $$\mathcal F$$来获取图片的feature map，这个feature map起到两个作用：(i) 在每个位置$$(i,j)$$的所有通道的值即为这个点的local descriptor $$d_{ij}$$；(ii) keypoints通过在feature map上使用一个non-local-maximum suppression，再在每个descriptor间使用一个non-maximum suppression来获得。keypoint detection score $$s_{ij}$$是通过一个soft local maximum score $$\alpha$$和一个每个descriptor的ratio-to-maximum score $$\beta$$计算得来的*

先利用某个预训练的网络来获取一个dense的feature map $$F = \mathcal F (I) \in \mathbb{R}^{h \times w \times n}$$，也叫做descriptors。利用每个像素点的descriptors之间的欧式距离可以很容易的计算两张图片之间的像素对应关系。在训练阶段，这些descriptors被训练为不同图片对应的同样的点具有相似的descriptors。每个descriptors被除以了它的$$L_2$$ norm来归一化这个descriptor。

而我们同时也可以将$$F$$看成一系列2D responses $$D$$的集合：$$D^k = F_{::k}, D^k \in \mathbb{R}^{h \times w}, \quad k=1,\cdots,n$$。在这个解释里，features $$\mathcal F$$可以被看成$$n$$个不同的2D的feature map，$$D^k$$，而它们则包含着keypoints的位置信息。这篇文章并不是简单的使用每个$$D^k$$获取一个keypoint，而是更复杂的设计。

对于使用传统的feature map，比如说DoG，来获取keypoints，我们会将该feature map通过一个空间的non-local-maximum suppression来稀疏化。但是在这篇文章的设定下，有多个feature maps $$D^k, k=1,\cdots,n$$，在他们中的任何一个上面都可以进行detection。因此，对于一个点$$(i,j)$$，如果它想要成为一个keypoint，则要满足：

$$(i,j) \quad \text{is} \  \text{a} \  \text{keypoint} \quad \iff D_{ij}^k \  \text{is} \  \text{a} \  \text{local} \  \text{max} \  \text{in} \ D^k, \quad \text{and} \ k = \arg\max\limits_{t} D{ij}^t$$

对于每个像素点$$(i,j)$$，上述设置会让我们在所有的detector $$D^k, k=1,2,\cdots,K$$里找到最显著的detector $$D^{k^{\ast}}$$（channel selection），然后再验证其在$$D^{k^{\ast}}$$上是否是一个局部最大值点。

在实践中，上述的hard detection procedure会被softened来满足back propagation。首先我们定义一个soft local-max score：

$$\alpha_{ij}^k = \frac{exp(D_{ij}^k)}{\sum_{(i^{'},j^{'}) \in \mathcal N(i,j)} exp(D_{i^{'}j^{'}}^k)}$$

其中$$\mathcal N(i,j)$$是像素点$$(i,j)$$周围包括自己一共9个点构成的集合。

然后我们再来定义soft channel selection，对于每个descriptor计算一个ratio-to-max来模仿每个通道的non-maximum suppression：

$$\beta_{ij}^k = D_{ij}^k / \max\limits_{t} D_{ij}^t$$

为了将上述两个criteria都考虑进来，我们对于所有的feature maps来最大化上述两个scores的积来获得一个简单的score map：

$$\gamma_{ij} = \max\limits_{k} (\alpha_{ij}^k \beta_{ij}^k)$$

最后，点$$(ij)$$的soft detection score $$s_{ij}$$通过一个image-level的归一化来得到：

$$s_{ij} = \gamma_{ij} / \sum_{(i^{'}, j^{'})} \gamma_{i^{'}j^{'}}$$

该方法只需要输入图片对之间ground truth的dense correspondence信息作为标签，并不需要keypoints的标签。



#### \[**ICCV 2019**\] [Joint Learning of Semantic Alignment and Object Landmark Detection](https://openaccess.thecvf.com/content_ICCV_2019/html/Jeon_Joint_Learning_of_Semantic_Alignment_and_Object_Landmark_Detection_ICCV_2019_paper.html)

![usr1]({{ '/assets/images/joint_1.png' | relative_url }}){: width=400px style="float:center"}

和[D2-Net: A Trainable CNN for Joint Description and Detection of Local Features](https://openaccess.thecvf.com/content_CVPR_2019/papers/Dusmanu_D2-Net_A_Trainable_CNN_for_Joint_Description_and_Detection_of_CVPR_2019_paper.pdf)
一样，本文将semantic correpondence学习和keypoint detection结合起来，输入是成对的图片，标签也只包含成对图片之间dense的correspondence，作者认为semantic correspondence的学习和keypoint detection这两个任务可以互相辅助。但因为没有keypoint的annotations，所以为了regularize由heatmaps得到的keypoints，作者还引入了$$\mathcal{L}_{\text{concentration}}$$和$$\mathcal{L}_{\text{separation}}$$来辅助学习。

> 注意，这篇文章从heatmaps提取keypoints的方式和其他的论文有所不同，对于网络输出的$$K+1$$个score map（还有个background的），注意score maps没有任何约束，可以取任何值。先对于每个像素点，沿着channel维度，计算softmax，得到$$K+1$$个heatmaps。然后对于每个表示keypoint的heatmap $$H_k$$，该heatmap获得的keypoint的计算方式为：$$\sum_{i=1}^H \sum_{j=1}^W (i,j) \times H_k(i,j) / \sum_{i=1}^H \sum_{j=1}^W H_k(i,j)$$。和别的方法的不同点在于，其并没有对于每个heatmap，在两个size维度进行softmax，而是在channel维度进行softmax。这样做的好处是可以使得检测到的keypoints天然的不易重复位置，但坏处是因为并没有沿着size维度进行softmax，可能会导致每个heatmap表示keypoint的位置不明显，影响精度。


#### \[**NeurIPS 2019**\] [Object landmark discovery through unsupervised adaptation](https://proceedings.neurips.cc/paper/2019/hash/97c99dd2a042908aabc0bafc64ddc028-Abstract.html)

[CODE](https://github.com/ESanchezLozano/SAIC-Unsupervised-landmark-detection-NeurIPS2019)

这篇文章的想法是，如果已经有了一个在某一个category上使用keypoint标注使用supervised的方法训练好的的keypoint detector（比如说就是之前的Hourglass network），那么对于新的category，是否可以保持该网络结构不变，只略微改变网络参数，就可以做到对这个新的category的图片的keypoint的detection。

其具体做法是对于Hourglass network来说（其包含pooling层、activation层、convolution层），只改变convolution层的参数。对于每一层，假设之前的网络参数是$$\Theta_{\mathcal{X}} \in \mathbb{R}^{C_o \times C_{in} \times k \times k}$$，其中$$C_o$$和$$C_{in}$$分别是输入输出channel维度，$$k$$是kernel size，那么我们对于该层只需要学习一个projection matrix $$W \in \mathbb{R}^{C_o \times C_o}$$，那么新的网络参数$$\Theta_{\mathcal{Y}}$$就可以表示为$$\Theta_{\mathcal{Y}} = W \Theta_{\mathcal{X}}$$，因为其维度和$$\Theta_{\mathcal{X}}$$相同，就可以直接使用原网络。

![usr1]({{ '/assets/images/adaptationkp_1.png' | relative_url }}){: width=400px style="float:center"}

而网络训练的损失函数，则和[Unsupervised Learning of Object Landmarks through Conditional Image Generation](https://proceedings.neurips.cc/paper/2018/hash/1f36c15d6a3d18d52e8d493bc8187cb9-Abstract.html)相同，对于每张输入$$y$$，获取其deformed之后的图片$$y^{'}$$，从而得到输入图片对$$\left{ y, y^{'} \right}$$。


#### \[**CVPR 2019** $$\&$$ **IJCV 2020**\] [DeepFlux for Skeletons in the wild](https://github.com/YukangWang/DeepFlux)


#### \[**NeurIPS 2020** $$\&$$ **TPAMI 2023**\] [Unsupervised Learning of Object Landmarks via Self-Training Correspondence](https://proceedings.neurips.cc/paper/2020/hash/32508f53f24c46f685870a075eaaa29c-Abstract.html)

[NeurIPS Version CODE](https://github.com/malldimi1/UnsupervisedLandmarks), [TPAMI Version CODE](https://github.com/malldimi1/KeypointsToLandmarks)

作者对于之前的unsupervised 2D keypoint inference方法的总结写的不错：

现有的无监督学习方法经常依赖于某些代理任务，而学习landmarks这样的一个主要任务通常是通过某种latent过程来实现的（比如说[Discovery of Latent 3D Keypoints via End-to-end Geometric Reasoning]）。还有一些方法使用某些代理任务，比如说equivariance：[Unsupervised learning of landmarks by descriptor vector exchange]，[Unsupervised learning of object frames by dense equivalent image labelling]，[Unsupervised learning of object landmarks by factorized spatial embeddings]，或者image generation：[Unsupervised learning of object landmarks through conditional image generation]，[Object landmark discovery through unsupervised adaption]，[Unsupervised discovery of object landmarks as structural representataions]。基于equivariance原则的方法表明，一个detector必须在经过已知的某种自定义的image deformation下保持连续（也就是说一张图片在经过image deformation前后，这个detector所检测到的keypoints要保证correspondence，其中这个image deformation是已知的），从而他们就利用这个原则来学习模型。而基于image generation的方法的流程是使用一个generator来reconstruct原输入图片，这个generator的输入是原图片经过了deformation之后的图片，而且这个generator是依赖于某个detector的输出的。detector和generator通过一个bottleneck来交换信息可以从输入里蒸馏出物体的几何信息，因为一个generator如果想从根据一张图片经过deformation之后的图片作为输入还能够reconstruct原输入照片，那它就需要对图片里物体的几何特征以及landmarks有足够的理解（也就是说，输入图片经过了某种deformation之后传给了generator，然后希望generator能够恢复原输入，这个时候generator所拿到的图片里的object的几何特征变了，但纹理特征没有，也就是说它需要找到deformation前后landmarks的对应关系来学习到这个deformation对于几何特征造成了什么样的影响， 才能够reconstruct原输入）。 

> 尽管这些方法在某些场景下取得了好的效果，但这些场景下物体一般都只有比较小的角度变化（比如说正面的人脸，人的身体，猫脸等），这些方法在以下两个角度上具有局限性。首先，代理任务并不能确保detector就能够显式的学习到物体的landmarks，因此会产生那些不太会被人类标注员所标注的landmarks。其次，这些方法都需要人为生成deformations，因为只有这样才能够产生一对图片，其landmarks之间的对应关系是知道的（deformation前后）。但是通过这样一对图片来学习（一张图片以及经过了deformation的这张图片）会导致模型对于更复杂的类内variation不具有鲁棒性，比如说背景变化、角度变化（经过了3D rotation）、或者说那些articulated的物体（比如说人的身体）。

> 在这篇文章里，作者观察到，尽管consistent的semantic的keypoints detector在无监督的方式下很难被训练，但从图片里获取一系列无序的没有correspondence的keypoints是很容易的（可以直接用Sobel filters（比如SIFT）或者预训练好的模型直接得到（比如SuperPoint）得到）。基于这个事实，作者提出了一个新的方法来从inconsistent的keypoints candidate set来获取consistent的keyponts。

作者发现，和先前的方法相比，本文提出的方法能够解决物体在进行很大角度变换之后的landmarks的对应关系，而且能解决对称的问题（也就是对称的物体的两个对称的landmark也是不同的），这一点从下图就能看出来。

![unsuper1]({{ '/assets/images/selftraining_1.png' | relative_url }}){: width=400px style="float:center"}

这篇文章对于无监督学习物体关键点提出了一种新的范式。和已有的利用image generation、equivariance或者autoencoder来进行无监督学习不同，本文提出的是一种自学习的方式，从general的keypoints candidates集合出发，来逐渐筛选并学习keypoints的detector和descriptor，逐渐将keypoints调整到更精确的位置上，将调整之后的点叫做landmarks。

> 为了避免误解，下面将不in correspondence的检测到的关键点叫做keypoints，将in correspondence的关键点叫做landmarks

具体来说，本文所提出的方法包括两个module：keypoint detector和keypoint descriptor。

对于keypoint detector来说，和之前的方法不同，作者在这里使用单个heatmap来预测所有的keypoints，也就是对于输入图片$$\text{I} \in \mathbb{R}^{H \times W \times 3}$$，输出heatmap为$$\text{H} \in \left( 0,1 \right)^{H_o \times W_o \times 1}$$，其中$$H_o,W_o$$和$$H,W$$等比例。而从heatmap获取keypoints的时候，使用的是非极大抑制（non-maximum suppression）的方法，具体来说就是，先对于设定的threshold，选出那些heatmap值大于threshold的positions。然后以每个position为中心，画出正方形方框（边长是一个hyperparameter），而该方框的score就是该position的heatmap的值，最后再结合一个IoU threshold（也是一个hyperparameter），使用`torchvision.ops.batched_nms`来获得最终的filter之后的点的集合（filter掉那些挨得太近的），这样对于每张图片得到的keypoint的数量并不是确定的，而且也并不in correspondence，将所有图片的所有检测到的这样的keypoints记为：$$\lbrace I_j, \lbrace p_i^j \rbrace_{i=1}^{N_k} \rbrace$$，其中$$p_i^j \in \mathbb{R}^2$$是一个keypoint二维坐标，$$N_j$$是图片$$I_j$$里所检测到的keypoints的数量。

对于keypoint descriptor来说，先对于输入图片，学习到一个dense的descriptor（也就是每个pixel都有个descriptor），$$F \in \mathbb{R}^{H \times W \times D}$$，然后根据上一步keypoint detector所得到的每张图片的keypoints集合，将每张图片每个keypoint对应pixel位置的descriptor取出来，将所有图片的所有按这样方式得到的features记为：$$\lbrace I_j, \lbrace f_i^j \rbrace_{i=1}^{N_k} \rbrace$$，其中$$f_i^j \in \mathbb{R}^D$$是一个keypoint二维坐标，$$N_j$$是图片$$I_j$$里所检测到的keypoints的数量。然后就到了关键步骤，对于所有的这些features，进行两次K-means的计算。具体来说，第一次K-means，将中心的数量设置为$$K$$（一个hyperparameter），而且对于每张图片来说，对于每个聚类中心，只保留一个keypoint（也就是说，如果对于某张图片，有两个该张图片检测到的keypoint都被分为到某个聚类中心$$C_k$$上，那么只保留距离该聚类中心最近的那个），因为只有这样才make sense（对于每个semantic的landmark，每张图片只应该检测到一个）。按照聚类的结果，我们可以filter掉一批keypoints（因为每张图片每个聚类中心只能有一个keypoint，也就是说现在每张图片至多保留$$K$$个keypoints）。第二次K-means，将中心的数量设置为$$M$$（也是一个hyperparameter，但是要远大于$$K$$，作者说这样设置的好处是可以account for因为大的角度变化所导致的features的不稳定，也正是因为这个原因，才能使得对于有大角度变化的数据集，检测到的landmarks仍然in correspondence），然后再次运行K-means算法，同样对于每个聚类中心，每张图片至多有一个keypoint能被标注为该聚类中心，从而现在的labels就有$$M$$类。

在训练的算法里，对于每个iterations来说，先使用keypoint detector获得每张图片的一个keypoints的集合，再使用keypoint descriptor得到对应的features的集合，再对于所有的训练数据（或者某一固定的部分）来作上述的clustering，从而对于keypoints进行filter，而且每张图片的每个keypoint都有了一个label（属于哪个聚类中心）。而损失函数包含两个部分：（1）每张图片filter之后的keypoints构成的联合Gaussian heatmap和keypoint detector输出的heatmap之间的差别，以及（2）使用聚类得到每张图片每个keypoint的label之后，基于keypoint features的contrastive loss（两张图片的同一label的keypoints的features相近，同一张图片的不同label的keypoints的features相远）。由该loss来对keypoint detector以及keypoint descriptor一起训练。

![unsuper2]({{ '/assets/images/selftraining_2.png' | relative_url }}){: width=400px style="float:center"}
*Fig 2. 上述所描述方法的流程图。所用到的网络是具有共用的backbone的两个输出头的网络，一个头是detector head，另一个是descriptor head。在训练过程中，算法将会在利用descriptor的features进行clustering从而给keypoints打上伪标签，与利用这种伪标签进行自学习，这两个过程之间反复横跳。和之前的那些方法都不同，本文所使用的方法并不需要一张图片以及它的deformation作为输入图片对，这种输入会减弱模型学习类内variation的能力。correspondence recovery是利用文中提出的修改版K-means实现的。本文的方法还可以用来恢复丢失的landmarks。上图还显示了使用t-SNE来可视化数据集里的keypoint对应位置的features的结果*

> 作者表示，需要将$$M$$设置为远大于$$K$$的值：这样会导致feature space的过分割问题，但这个却可以使得每个landmark可以对应好几个clusters，这样带来的好处就是它可以解决图片中物体视角变化非常大的情况，因为由于视角变化，实际上这两张图片里的同样区域的clusters的features不太一样，但它们都被贴在了同一个landmark上，可以由后续操作将它们合并。这一点也是本文的方法和之前的工作的一个不同，本文的方法可以解决角度变化很大的情况。

> 因为该算法流程是在keypoint detection和利用keypoint descriptor和clustering进行labelling这两个module之间iterative进行的，所以就需要一个起始值。这篇文章使用SuperPoint来对每张图片获取初始的keypoint candidates set。而且其还对keypoint descriptor的初始化进行了一些warm up的处理。具体来说，最一开始，在$$t=0$$的时刻，训练集$$X_0$$仅仅包含$$\lbrace x_j, \lbrace p_i^j \rbrace_{i=1}^{N_j} \rbrace$$，而并没有keypoint之间的correspondence $$f_i^j$$。在NeurIPS的版本里，最初始的features是由某个预训练好的网络给的（SuperPoint），然后就可以进行后续的循环了。但在TPAMI的版本里，作者采用了一个warm up的预训练stage，也就是只利用一张图片和它的deformation构成的图片对作为输入来训练网络。这样的话keypoints之间的对应关系就已经知道了，这就是positive pairs。也就是利用equivariance性质来初始化backbone和feature extractor head。

在上述流程循环了足够多次之后，我们所得到的结果是对于每张图片，都可以得到一个keypoint set，而且这些keypoints也都有labels（$$M$$个类），以及keypoints对应的descriptors。但我们的终极目标是，对于每张图片，输出固定数量（$$K$$个）landmark坐标。

所以，将上述算法称为stage1，而我们还需要在stage1结束之后，增加一个stage2。

现在keypoints的labels数量远大于$$K$$（$$M$$个），但我们并不知道到底哪些clusters需要对应到同一个landmark。在NeurIPS那个版本里，这个过程是通过逐渐合并clusters来减少clusters数量到$$K$$个来实现的。然而，因为现在每张图片的keypoints数量最多是$$K$$个，也因为采用了更好的negative pair的选取策略，作者发现即使有$$M$$个clusters，这些clusters也自动形成了$$K$$个分离的很好的clusters。这个事实就使得我们并不再需要在NeurIPS里所使用的逐渐合并clusters的这样一个流程，只需要再次使用了具有$$K$$个聚类中心的K-means就可以了。之后就可以根据由stage1得到的标注以及这里对标注从$$M$$个到$$K$$个的合并，重新训练一个和supervised的方法一样的输出$$K$$个heatmap的keypoint detector了，这就是stage2。

#### \[**CVPR 2020 Oral**\] [Self-supervised Learning of Interpretable Keypoints from Unlabelled Videos](https://www.robots.ox.ac.uk/~vgg/research/unsupervised_pose/data/unsupervised_pose.pdf)

[POST](https://www.robots.ox.ac.uk/~vgg/research/unsupervised_pose/)

这篇文章的效果非常好（更多的图可以去项目主页看到）：

![unsuper1]({{ '/assets/images/unlabelvideokp_1.png' | relative_url }}){: width=400px style="float:center"}
*Fig 1. 从上图可以看到，即使是左右翻转的情况，keypoints的对应关系仍然是正确的，即并没有将左手识别为右手，右手识别为左手*

效果这么好的原因有两个：（1）使用了skeleton来表示structure，而不仅仅是keypoints；（2）使用了未配对的ground truth的keypoint和skeleton标注

这篇文章仍然是基于[Unsupervised Learning of Object Landmarks through Conditional Image Generation](https://proceedings.neurips.cc/paper/2018/hash/1f36c15d6a3d18d52e8d493bc8187cb9-Abstract.html)，即输入仍然是一对描述同一个物体的不同图片，$$\lbrace \pmb{x}, \pmb{x}^{'} \rbrace$$（在这里就是一个视频的两帧），并且由detector $$\Phi$$来对输入$$\pmb{x}$$预测其的某种structure representation $$\Phi(\pmb{x})$$，然后和$$\pmb{x}^{'}$$一起作为decoder $$\Psi$$的输入，来reconstruct $$\pmb{x}$$。在之前的文章里，该structure representation是由Gaussian的heatmaps堆叠而成的，但在这篇文章里，其是skeleton，记为$$p$$。具体来说，$$p = \Phi(\pmb{x})$$。但没有任何限制的话，$$p$$可以是任意的形式，所以其设计为：$$p = \beta(\eta(\Phi(\pmb{x})))$$，其中$$\eta$$是将skeleton映射为keypoint matrix $$M \in \mathbb{R}^{K \times 2}$$的函数（由neural network实现），而$$\beta$$是将keypoint matrix $$M$$映射为skeleton $$p \in \mathbb{R}^{H \times W}$$的函数。

$$\beta$$的具体设计为，对于某个预先设定好的limbs set $$E$$（$$E$$里是keypoints pairs）、某个temperature hyperparameter $$\gamma$$，$$H \times W$$里的任意一个位置$$u$$的值$$p(u)$$为：

$$p(u) = exp(-\gamma \min\limits_{(i,j) \in E, r \in \left[0, 1 \right]} \lVert u - rp_i - (1-r)p_j \rVert_2^2)$$

直观来说，也就是以$$E$$里那些limbs构成的联合高斯分布。

本文的视觉效果那么好的第二个原因是它们直接使用了标注好的keypoints和skeleton的ground truth。但其并没有对图片直接进行标注，这样的标注是和数据集内任何一张图片都不匹配的，也就是说有$$M$$个这样的$$p$$构成的集合：$$\lbrace p_j \rbrace_{j=1}^M$$。

> 实际上也就是给了shape template

作者说之所以skeleton比keypoint构成的Gaussian heatmap的效果要好，是因为让$$\Phi(\pmb{x})$$输出skeleton而不是输出离散的keypoint matrix这样一个任务类似于image-to-image translation（skeleton可以理解为一张灰度图），而这个任务更加适合神经网络去学习。

而且为了让$$\Phi(\pmb{x})$$能更好的输出skeleton，以及为了利用那$$M$$个ground truth，作者引入了一个discriminator $$D$$，将$$\Phi$$当作generator，来生成尽可能和那$$M$$个ground truth相似的skeleton。

![unsuper2]({{ '/assets/images/unlabelvideokp_2.png' | relative_url }}){: width=400px style="float:center"}



#### \[**NeurIPS 2022 Spotlight**\] [AutoLink: Self-supervised Learning of Human Skeletons and Object Outlines by Linking Keypoints](https://arxiv.org/pdf/2205.10636.pdf)

[CODE](https://github.com/xingzhehe/AutoLink-Self-supervised-Learning-of-Human-Skeletons-and-Object-Outlines-by-Linking-Keypoints)

这篇文章要解决的问题是无监督的从一类物体的图片里学习到2D keypoints。模型的输入是一类物体（比如人脸）的RGB图片，输出是2D keypoints，数量和顺序是固定的。也就是说，模型在输入图片后，输出$$K$$个heatmap，然后从$$K$$个heatmap里获取$$K$$个2D keypoints。所以不同图片的2D keypoints之间的correspondence也是直接就有的。

很多无监督2D keypoints的方法都是一个auto encoder的结构，但这篇文章的创新点有如下几个：

* 首先，网络除了输出$$K$$个heatmap之外，还会输出一个$$K \times K$$的graph，用来表示没两对keypoints之间的权重，而且这个graph是global的，也就说对于每个输入图片，其是共同的。
* 其次，对于每对输出的keypoints，构建一个edge heatmap，大小和输入图片一样，是以这两个keypoint的连线为中心的Gaussian分布，再利用上面的graph，得到每个像素点位置的global edge heatmap的值
* 在得到这个global edge heatmap之后，还是一样需要一个decoder来reconstruct原输入图片，显然只有这个骨架是没办法还原的，所以文章的做法是对于输入图片，mask掉其绝大部分区域，和这个edge heatmap沿着channel连起来，作为decoder的输入。

> 实际上AutoLink基于auto encoder结构，且思路类似于[Self-supervised Learning of Interpretable Keypoints from Unlabelled Videos](https://www.robots.ox.ac.uk/~vgg/research/unsupervised_pose/data/unsupervised_pose.pdf)，但AutoLink将整个框架变成了一个unsupervised的流程，这是其的主要贡献

这篇文章的方法很简单，却很有效果。

有个要注意的技术细节是，reconstruction loss不仅仅是reconstructed的图片和原图片之间的mse loss，而是perception loss，也就是将这两个图片都输入某个预训练好的网络，比如在ImageNet上预训练好的VGG19，然后对比很多层的输出之间的差异的和。这样做要比仅仅在像素层面比较区别更加鲁棒。


#### \[**CVPR 2024 Highlight**\] [Unsupervised Keypoints from Pretrained Diffusion Models](https://openaccess.thecvf.com/content/CVPR2024/papers/Hedlin_Unsupervised_Keypoints_from_Pretrained_Diffusion_Models_CVPR_2024_paper.pdf)

*Eric Hedlin, Gopal Sharma, Shweta Mahajan, Xingzhe He, Hossam Isack, Abhishek Kar, Helge Rhodin, Andrea Tagliasacchi, Kwang Moo Yi*

[CODE](https://github.com/ubc-vision/StableKeypoints)

这篇文章和[Unsupervised Semantic Correspondence Using Stable Diffusion](https://ubc-vision.github.io/LDM_correspondences/)一样，基于的想法是，对于训练好的text-based StableDiffusion模型，即使是random的text embedding，和multi-head encoder得到的image embedding计算相似度再可视化，都能看到图中object大致的形状，并且相似度高的区域具有一定的semantics。

所以本文的想法是，对于预训练好的StableDiffusion，optimize若干个text embedding，使得每个text embedding和image embedding计算完相似度之后（这个相似度matrix就可以理解成一个heatmap），该heatmap是单峰的，这也就意味着该text embedding对应着所有图片的某一固定区域（比如眼睛），如下图所示：

![unsuper1]({{ '/assets/images/stablekeypoints_1.png' | relative_url }}){: width=800px style="float:center"} 

具体来说，假设transformer有$$L$$层，每一层有$$C$$个head，那么每一层就可以得到$$C$$个pixel-level的image embedding：$$F_{I}^c \in \mathbb{R}^{H \times W \times D}$$，其中$$c=1,2,\cdots,C$$，$$H,W$$是图片尺寸，$$D$$是特征维度。对于text-embedding来说，假设有$$N$$个token，这样所有的token的embedding集合是$$F_{T} \in \mathbb{R}^{N \times D}$$。从而就可以计算pixel-level的text和image pixel的相似度：$$M_c \mathbb{R}_{+}^{H \times W \times N}$$。最后的相似度矩阵$$M$$是先将$$M_c$$沿着$$N$$维度作softmax，再对于所有的head进行一个average，最后再在$$L$$层上选出几层出来average。

在得到了相似度矩阵$$M \mathbb{R}_{+}^{H \times W \times N}$$之后，其loss是对于每一个$$M_i \mathbb{R}_{+}^{H \times W \times N}$$，$$i=1,2,\cdots,N$$，先找到该$$M_i$$最大值所在的坐标，以该坐标为中心，在$$H \times W$$的2维grid上构建一个高斯分布（方差是超参数），然后计算$$M_i$$与该高斯分布之间的损失。

有几点值得注意的技术细节：
* 首先，在训练的时候，除了上述的损失函数，还加上了equivariance损失，输入图片对是通过手动制作transformation实现的
* 其次，因为keypoints存在遮挡的情况，所以对于某些图片来说，可能计算出来的相似矩阵是比较弥散的（即不能对应一个尖峰高斯，因为其本来就检测不到）。为了解决这个问题，作者提出的engineering的方法是先设定一个较大的keypoint值，比如说50，最后选择那些和高斯分布的KL散度最小的相似矩阵对应的那些keypoint作为输出，比如说25
* 最后，为了实现所得到的keypoints确实能够覆盖object，作者还在上一步所得到的那些keypoint里，继续使用fartest point sampling选择出一个subset作为最终的输出，比如说15

和autolink相比，本文的方法对于pose比较大的情况，确实能够正确识别，比如下图：

![unsuper1]({{ '/assets/images/stablekeypoints_2.png' | relative_url }}){: width=800px style="float:center"} 

但是对于某些情况，比如说人体，该方法仍然不能正确的区分正反面（可能是因为人脸这种用于区分正反面的特征在图片里的区域太小了，导致特征信息不够）：

![unsuper1]({{ '/assets/images/stablekeypoints_3.png' | relative_url }}){: width=800px style="float:center"} 

> 而且本文的另一个缺点是，其是针对整个数据集的所有图片找到它们**最公共**的$$K$$个keypoints输出，这样对于某些数据集，比如CUB_200_2011来说，其绝大多数图片都是鸟的两个侧面之一，所以最终输出的$$K$$个keypoints都是侧面的这些点，但对于偶尔出现的鸟的俯视或者正视图，这$$K$$个keypoints里的有些点在图中都没有ground truth的对应

> 而且实际上，StableKeypoints的textual tokens还是表示的是local信息，因为从CUB_200_2011或者是Human3.6m的结果来看，其并不能分清左右，即左手和右手，或者左眼和右眼，它们的features是一样的，对应同一个textual token


### 2D Keypoint Refinement

#### \[**ECCV 2024**\] [GMM-IKRS: Gaussian Mixture Models for Interpretable Keypoint Refinement and Scoring](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/09846.pdf)

#### \[**ECCV 2024**\] [ConDense: Consistent 2D/3D Pre-training for Dense and Sparse Features from Multi-View Images](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/07038.pdf)

### 2D Keypoint Applications

#### \[**Arxiv 2022**\] [LatentKeypointGAN: Controlling GANs via Latent Keypoints](https://xingzhehe.github.io/LatentKeypointGAN/)

## 3D Keypoint Detection

### Supervised 3D Keypoint Detection

#### \[**ICCV 2017**\] [How far are we from solving the 2D & 3D Face Alignment problem?](https://github.com/1adrianb/2D-and-3D-face-alignment)

[CODE1](https://github.com/1adrianb/2D-and-3D-face-alignment), [CODE2](https://github.com/1adrianb/face-alignment)

#### \[**NeurIPS 2018 Oral**\] [Occlusion-Net: 2D/3D Occluded Keypoint Localization Using Graph Networks](https://openaccess.thecvf.com/content_CVPR_2019/papers/Reddy_Occlusion-Net_2D3D_Occluded_Keypoint_Localization_Using_Graph_Networks_CVPR_2019_paper.pdf)

[CODE](https://github.com/dineshreddy91/Occlusion_Net)

![unsuper1]({{ '/assets/images/occlusion_1.png' | relative_url }}){: width=800px style="float:center"} 

这篇文章的输入是RGB图片，标注信息包括$$K$$个keypoint的2D image coordinate，以及这$$K$$个keypoint的visibility的情况（0或者1），并且这$$K$$个keypoints的顺序需要确保（是有序序列，等价于keypoints有labels）。

> 也就是说，本文的输入setting和C3DPO等NrSfM方法的setting是一样的

本文的总体思路是，使用Graph Neural Network来对所有的$$K$$个keypoint以及它们之间的edges进行建模，从而可以用每张图片的visible keypoints来预测invisible keypoints。keypoints之间的edges就是0或者1（如果两个keypoints在该张图片都可见，就是1，其他情况是0）。

具体来说，本文使用的是监督学习的方法，框架包括三个部分：2DKNN Encoder，2DKNN Decoder以及3DKNN Encoder。

首先，使用Mask-RCNN从输入图片中输出$$K$$个heatmap来获得$$K$$个keypoints的坐标$$(x_i, y_i), i=1,2,\cdots,K$$，同时也使用heatmaps来获得每个keypoint的confidence $$c_i,i=1,2,\cdots,K$$，再加上该keypoints的ground truth visibility $$t_i, i=1,2,\cdots,K$$，共同作为GNN的每个node的输入features $$\mathcal{V}_i = (x_i,y_i,c_i,t_i)$$，输入给2DKNN Encoder来使用GNN得到edge的prediction。此处有两个losses：由heatmaps得到的keypoints坐标和ground truth坐标之间的差距（只考虑visibility ground truth可见的那些点），预测得到的edges和ground truth edges之间的差距（ground truth edges由ground truth visibility计算得到）。

之后，将由2DKNN Encoder更新后的graph输入给2DKNN Decoder，来进一步优化$$K$$个keypoints的坐标位置。但是这一步的loss仅计算ground truth occluded的keypoints（即visibility ground truth是0的那些点）和预测keypoints之间的差异，而且该差异是通过对于每张照片，使用其同一个场景另外两个views，利用每个keypoint在这三张照片里的trifocal tensor的trilinear constraints来计算的loss。

最后，再将由2DKNN Decoder更新后的graph输入给3DKNN Encoder，输出basis和camera pose estimation $$Pi$$，和C3DPO相同，用bases表示3D keypoints，然后使用$$Pi$$将其project到2D相机平面上，最后计算projected的3D keypoints和ground truth的2D keypoints之间的差距（注意是所有的keypoints，不考虑可不可见，都算上）。

最后将这四个losses作为最终的训练loss来同时优化这三个部分。

> 本文的实验是在一个car dataset上做的，该dataset背景复杂，且可能含有多个cars，但car本身是rigid objects，所以该数据集有它的难度，但在rigidity上变化较小。而且具体操作的时候，每次是将每张照片按照ground truth的bounding box裁剪掉多余的背景之后再做为输入，也就是每次只有一个center positioned的car，如上面流程图所示。

![unsuper1]({{ '/assets/images/occlusion_2.png' | relative_url }}){: width=800px style="float:center"} 


#### \[**ICLR 2021**\] [Semi-supervised Keypoint Localization](https://olgamoskvyak.github.io/files/paper5.pdf)

[CODE](https://github.com/olgamoskvyak/tf_equivariance_loss)


#### \[**ICLR 2022**\] [Pseudo-labeled Auto-curriculum Learning for Semi-supervised Keypoint Localization](https://openreview.net/pdf?id=6Q52pZ-Th7N)




#### \[**CVPR 2023**\] [Few-shot Geometry-Aware Keypoint Localization](https://xingzhehe.github.io/FewShot3DKP/)

这篇文章提出的是一种semi-supervised的方法来学习2D keypoints，其输入为某个类别的图片集合，其中一小部分图片标注有ground truth的$$K$$个keypoints的annotations，但不论是否visible，都标记上了（也就是没有visibility的标注），而且还有物体的keypoints edges linkage的信息，以及物体的keypoints如何被分为几个subparts的信息。

![unsuper1]({{ '/assets/images/fewshot_1.png' | relative_url }}){: width=800px style="float:center"} 

该方法的框架是基于AutoLink的，其使用$$K$$个heatmaps来获得$$K$$个keypoints，然后根据keypoints来构造edge map（每对keypoints构成的edge map含有一个可学习的系数），并且将原图片randomly masked的features与edge map结合输入给decoder来reconstruct原输入图片，并且设计一个reconstruction loss，这些都与AutoLink相同。不同的是，类似于[Unsupervised learning of object landmarks by factorized spatial embeddings](https://openaccess.thecvf.com/content_ICCV_2017/papers/Thewlis_Unsupervised_Learning_of_ICCV_2017_paper.pdf)，本文也使用了equivariance loss。并且网络不仅输出了$$K$$个heatmaps，还输出了$$K$$个uncertainty maps $$V_i \in \left(0, 1 \right)^{H \times W}, \ i=1,2,\cdots,K$$，并且最终的每个keypoint的uncertainty由经过softmax的heatmap $$H_i$$与uncertainty map $$V_i$$进行element-wise加权求和而来。这个uncertainty $$v_i$$被用在edge map的计算里。并且，网络还对于每个keypoint都输出了一个dense的depth map $$D_i \in \mathbb{R}^{H \times W}$$，每个keypoint的depth也是由$$D_i$$和对应的heatmap的softmax加权平均而来，每个3D keypoints的坐标就是由heatmap得到的2D坐标扩展为3维（扩展的维度的值就是该点的depth）。而对这些3D keypoints的约束则是对于每个3D keypoints set，大小为$$K \times 3$$，先按照物体预定义好的subparts，将其分为$$\mathcal{P}$$个部分，对于每个部分，对于每对随机选择的两张图片$$\pmb{I}, \pmb{I}^{'}$$，有两组该部分的3D keypoints，$$P_{\pmb{I}}^j \in \mathbb{R}^{N_j \times 3}$$，$$P_{\pmb{I}^{'}}^j \in \mathbb{R}^{N_j \times 3}$$，$$j=1,2,\cdots,\mathcal{P}$$，其中$$N_j$$表示第$$j$$个部分点的数量。计算$$P_{\pmb{I}}^j$$和$$P_{\pmb{I}^{'}}^j$$之间的procrustes analysis，然后将aligned之后的$$P_{\pmb{I}}^j$$和$$P_{\pmb{I}^{'}}^j$$之间的距离，按照$$\mathcal{P}$$个部分求平均，称为3D geometry loss。最后，对于那些有标注的图片，还有个supervised的loss。

![unsuper1]({{ '/assets/images/fewshot_2.png' | relative_url }}){: width=800px style="float:center"} 

#### \[**CVPR 2024**\] [3D-LFM: Lifting Foundation Model](https://3dlfm.github.io/)


### Unsupervised 3D Keypoint Detection

#### \[**ECCV 2018**\] [Unsupervised Domain Adaptation for 3D Keypoint Estimation via View Consistency](https://github.com/xingyizhou/3DKeypoints-DA)

[CODE](https://github.com/xingyizhou/3DKeypoints-DA)

这篇文章的想法和那篇unsupervised 2D keypoint detection的论文[Object landmark discovery through unsupervised adaptation](https://proceedings.neurips.cc/paper/2019/hash/97c99dd2a042908aabc0bafc64ddc028-Abstract.html)类似，都是让网络在某个类别的source datasets上supervised训练，然后将这个网络参数微调，使得其能够适应新的另一个类别的target dataset。但这里的数据是3D keypoints。

这篇文章的数据集包含了丰富的信息。数据集构成为：一个source datasets，每个数据点都有RGB图片$$\pmb{x}$$以及对应的3D keypoint matrix $$P \in \mathbb{R}^{K \times 3}$$；一个target datasets，其包含$$N$$个场景，而每个场景又包含$$M_j$$个multi views，$$j=1,2,\cdots, N$$。

具体做法是，使用一个神经网络$$\Phi$$来使用图片作为输入，输出3D keypoint matrix，损失函数包括三个：（1）在source datasets上预测的3D keypoints和ground truth之间的差异；（2）在target dataset上，对于每个场景，同时也optimize一个$$M_i \in \mathbb{R}^{K \times 3}$$，代表这个场景在canonical frame下的3D keypoints的坐标，然后对于这个场景的每个view $$I_j, j=1,2,\cdots,M_j$$，先得到3D keypoints的输出$$X_{I_j} = \Phi(I_j) \in \mathbb{R}^{K \times 3}$$，然后使用procrustes analysis将$$X_{I_j}$$align到$$M_i$$上去，计算aligned之后的$$X_{I_j}$$和$$M_i$$之间的距离，即为multi-view loss；（3）最后一个loss用来减小网络对于source dataset和target dataset输出之间的差异，具体做法是，对于每个$$M_i$$，其对source datasets里的每个ground truth $$Y_j$$都做procrustes analysis，然后计算aligned后的$$M_i$$和这些$$Y_j$$之间的距离，选择最小的那个作为该$$M_i$$和source datasets之间的距离，然后对所有的$$M_i$$做上述操作，将距离求和，即为chamfer loss。

> 该方法直接利用神经网络以图片为输入，每次都输出所有的3D keypoints，没有考虑visibility

#### \[**NeurIPS 2018 Oral**\] [Discovery of Latent 3D Keypoints via End-to-end Geometric Reasoning](https://keypointnet.github.io/keypointnet_neurips.pdf)

[Review](https://papers.nips.cc/paper/2018/file/24146db4eb48c718b84cae0a0799dcfc-Reviews.html)
[CODE1](https://github.com/ZhengyiLuo/KeypointNet), [CODE2](https://github.com/ihankyang/keypoint-network), [CODE3](https://github.com/Noone65536/KeypointNet_network)

对于使用无监督的方式来获取keypoints，本文的思路是将这些keypoints要用在什么下游任务这个信息考虑进来。利用下游任务来优化keypoints的选择会自然的鼓励网络学到对于这个下游任务最有用的那些keypoints。这篇文章提出的KeypointNet是一个end-to-end的geometric reasoning框架，对于某个特别的下游任务，学习一个最优的category-specific以及task-specific 3D keypoints集合。该方法的创新点在于我们引入一个任意的下游任务来将3D keypoints作为latent variable学习，从而做到无监督。该方法对于任何目标函数相对于3D keypoints positions是可微分的下游任务都是可以使用的。

本文使用了3D pose estimation作为下游任务，即目标函数旨在为恢复同一个object的两个views之间的relative pose。

> 3D pose estimation的标准做法是：1）先检测一个稀疏的category-specific keypoints的集合；2）之后再利用这些keypoints使用某种geometric reasoning框架（比如说，PnP algorithm）来重建3D pose或者相机角度。

![PIPELINE]({{ '/assets/images/DOWNSTREAM-2.PNG' | relative_url }}){: width=800px style="float:center"}
*在训练过程中，同一个object的两个views都会给KeypointNet作为输入。两个views之间的rigid transformation $$(R,t)$$也作为监督信号被提供。我们学习一个3D keypoints的ordered list，在两个views里是consistent的，而且能够让我们复原transformation。在inference的时候，KeypointNet从一张单独的输入图片提取3D keypoint positions*

训练集的输入包括一对图片$$(I,I^{'})$$，其是同一个object的两个不同角度的图片，以及它们的relative rigid transformation $$T \in SE(3)$$。我们将通过优化一个目标函数$$O(f_{\theta}(I), f_{\theta}(I^{'}))$$，来学习一个函数$$f_{\theta}(I)$$，参数是$$\theta$$，将一个2D的图片$$I$$映射到一个3D keypoint locations的list，$$P=(p_1,p_2,...,p_N)$$，其中$$p_i=(u_i,v_i,z_i)$$。

函数$$f_{\theta}(I)$$由神经网络来实现，其输出是$$N$$个heatmap，$$g_i(u,v)$$，表示的是keypoint i在pixel位置$$(u,v)$$地方可能出现的概率，从而$$\left[ u_i, v_i \right]^T = \sum_{u,v} \left[ ug_i(u,v), vg_i(u,v) \right]^T$$

对于$$z$$ coordinates来说，网络$$f_{\theta}(I)$$还输出了$$N$$个深度的heatmap，$$d_i(u,v)$$，从而$$z_i = \sum_{u,v} d_i(u,v) g_i(u,v)$$

该方法使用了两个loss：（1）用来衡量两个views的3D keypoints matrix的multi-view consistency loss，其目的是确保3D keypoints在不同的views下仍然consistent，即两个views里的同一个3D keypoint应该在3D空间里对应到真实object的同一个点；（2）由下游代理任务引入的loss，在本文里是3D pose estimation loss

对于multi-view consistency loss来说，假设已知相机的global focal length $$f$$，使用$$\left[ x,y,z \right]$$代表world coordinate，使用$$\left[ u,v \right]$$代表image coordinate，并且使用perspective projection来作为2D-3D transformation。假设从$$f_{\theta}(I)$$输出所得到的第$$i$$个3D keypoint的inhomogeneous coordinate是$$\left[ x_i, y_i, z_i, 1 \right]$$，那么该点在$$I$$上的2D投影为：$$\left[u_i, v_i, 1 \right] = \left[ fx/z, fy/z, 1 \right]$$。假设从$$f_{\theta}(I^{\prime})$$输出所得到的第$$i$$个3D keypoint的inhomogeneous coordinate是$$\left[ x_i^{\prime}, y_i^{\prime}, z_i^{\prime}, 1 \right]$$，而$$I$$和$$I^{\prime}$$之间ground truth的transformation是$$T \in SE(3)$$，从而该点经过transformation $$T$$之后（为$$T \left[ x_i, y_i, z_i, 1 \right]^T$$）在$$I$$上的2D投影记为：$$\left[\hat u_i^{\prime}, \hat v_i^{\prime}, 1 \right]$$。同理定义$$\left[u_i^{\prime}, v_i^{\prime}, 1 \right]$$和$$\left[\hat u_i, \hat v_i, 1 \right]$$。从而symmetric multi-view consistency loss定义为：

$$\mathcal{L}_{con} = \frac{1}{2N}\sum_{i=1}^N \lVert \left[ u_i, v_i, u_i^{\prime}, v_i^{\prime} \right]^T - \left[ \hat u_{i}^{\prime}, \hat v_{i}^{\prime}, \hat u_{i}, \hat v_{i} \right]^T \rVert^2$$

3D pose estimation loss用来衡量预测的relative rotation $$\hat{R}$$（用两个3D keypoints matrix利用Procrustes'alignment计算得来）和ground truth rotation $$R$$之间的差异：

$$\mathcal{L}_{pose} = 2arcsin(\frac{1}{2\sqrt{2}} \lVert \hat{R}-R \rVert)$$

> 作者说，KeypointNet具有translation equivariance性质，并且已经有了上述的multi-view consistency loss，所以就不对translation的真实值和预测值进行约束

使用Procrustes'alignment来估计$$\hat{R}$$的方法如下。记$$X$$和$$X^{\prime} \in R^{3 \times N}$$为两个views在camera coordinates system下的3D keypoint locations。也就是说，$$X = \left[X_1,X_2,...,X_N \right]$$。$$\tilde{X}$$和$$\tilde{X}^{\prime}$$表示$$X$$和$$X^{\prime}$$分别将各自的mean（大小为$$3$$）减掉之后的矩阵。$$\hat{R}$$就用以下的方式计算而得：

$$\hat{R} = Vdiag(1,1,...,det(VU^T))U^T$$

其中$$U, \Sigma, V^T = SVD(\tilde X \tilde X^{\prime T})$$。

> 为了确保$$\tilde{X}\tilde{X}^{\prime T}$$是invertible的，且为了增强3D keypoints的鲁棒性，作者还在$$X$$和$$X^{\prime}$$内加入了Gaussian噪声。

> 上述计算rotation $$\hat{R}$$的问题框架叫做orthogonal Procrustes problem

> 对于很多下游的任务来说，还有很多常见的keypoints的性质可以起到帮助的作用，比如说（1）Separation loss可以对两个靠的太近的3D keypoints进行惩罚：$$\mathcal{L}_{sep} = \frac{1}{N^2}\sum_{i=1}^{N}\sum_{j \neq i}^N \max(0, \delta^2- \lVert X_i- X_j \rVert^2)$$；（2）Silhouette consistency鼓励keypoints落在object的轮廓内。我们网络对于第$$i$$个keypoint的position的预测是通过计算$$g_i$$（也就是heatmap）的期望得来的。一个确保silhouette consistency的方法就是只在silhouette内让概率非零，记图片的mask为$$b(u,v) \in \lbrace 0,1 \rbrace$$。从而$$\mathcal{L}_{obj} = \frac{1}{N}\sum_{i=1}^N -log\sum_{u,v}b(u,v)g_i(u,v)$$；（3）使得表示2D keypoint位置的distribution峰比较尖：$$\mathcal{L}_{var} = \frac{1}{N}\sum_{i=1}^N \sum_{u,v} g_i(u,v) \lVert \left[ u,v \right] - \left[ u_i, v_i \right] \rVert^2$$，



#### \[**ICML 2021**\] [Unsupervised Learning of Visual 3D Keypoints for Control](http://proceedings.mlr.press/v139/chen21b/chen21b.pdf)

[Code](https://github.com/buoyancy99/unsup-3d-keypoints) [Post](https://buoyancy99.github.io/unsup-3d-keypoints/)

这篇文章的假设前提是：有$$N$$个已经标定好的相机（也就是extrinsic和intrinsic都是知道的）来产生统一个场景的$$N$$个views，然后从这$$N$$个views来无监督的获取3D keypoints

> 但实际上相机内外参给定的情况下，很多信息都给了

给定同一个场景的$$N$$个view，我们为每个view都给一个encoder和一个decoder。

$$I_n \in \mathbb{R}^{H \times W \times C}$$表示相机$$n$$的输入image，$$n \in 1,\cdots, N$$，记相机$$n$$的extrinsic matrix为$$V_n$$，intrinsic matrix为$$P_n$$。$$K$$是keypoints的个数。对于一个点的world coordinate system下的坐标$$\left[ x,y,z \right]^T$$和相机$$n$$，我们可以使用extrinsic matrix $$V_n$$和intrinsic matrix $$P_n$$来将其投射到camera coordinate system下的坐标$$\left[ u, v, d \right]^T$$，其中$$u, v \in \left[ 0,1 \right]$$是camera plane上归一化后的coordinate（这里将camera coordinate system和image coordinate system结合在一起了），$$d>0$$是depth value，也就是该点距离camera plane的距离。将上述的这种投射记为，operator $$\Omega_n: \left[x,y,z\right]^T \rightarrow \left[u,v,d\right]^T$$，而其inverse记为$$\Omega_n^{-1}$$。$$\Omega_n, \Omega_n^{-1}$$都是differentiable的。

![Model Structure]({{ '/assets/images/CONTROL-1.PNG' | relative_url }}){: width=800px style="float:center"}
*Fig 1. Overview of our Keypoint3D algorithm. (a) 对于每个camera view，一个CNN将输入image编码为$$K$$个heatmaps以及depth maps；(b) 我们将这些heatmaps当作概率来计算camera plane下keypoint横纵坐标的。我们同时也用heatmap和depth map来计算每个keypoint的深度d，这些$$\left[u,v,d\right]$$再被反投射回到world coordinate里；(c) 我们利用之前的heatmaps来计算每个camera view对于每个keypoint的置信概率，然后对于每个keypoint，我们计算出一个加权的world coordinate；(d) 我们再将每个keypoint的world coordinate投射到每个camera plane上；(e) 从而对于每个camera plane，都有$$K$$个这样的投射，建立$$K$$个高斯map，将它们叠起来，作为decoder的输入，来重构原输入图片；(f) 除了上述的这些loss，我们还将所学习到的3D keypoint的world coordinate与下游任务相结合，来共同优化这个网络。*

具体来说，分以下四步来实现：

**Step1 Keypoint Encoder**

对于每个camera $$n$$，我们将$$I_n$$喂给一个fully convolutional encoder $$\phi_n$$来获得$$k$$个confidence maps，$$C_n^k \in \mathbb{R}^{W \times H}$$，以及depth maps，$$D_n^k \in \mathbb{R}^{W \times H}$$。对于每个confidence map，我们使用一个spatial softmax来计算得到probability heatmap $$H_n^k(i,j) = \frac{\text{exp}(C_n^k(i,j)}{\sum_{p=1}^W \sum_{q=1}^H \text{exp}(C_n^k(p,q))}$$

每个3D keypoint $$k$$在相机$$n$$的camera coordinate system下的坐标即为：

$$\left[\hat u_n^k, \hat v_n^k, \hat d_n^k \right] = \left[ \frac{1}{W} \sum_{u,v} u H_n^k(u,v), \frac{1}{H} \sum_{u,v} v H_n^k(u,v), \sum_{u,v} u D_n^k(u,v) H_n^k(u,v) \rigjt]$$

> 注意到，对于$$u,v$$的计算都除以了图片尺寸，是因为计算的是归一化之后的相机坐标下的3D keypoint坐标。


**Step2 Attention**

在预测了每个camera coordinate frame下每个3D keypoint的坐标之后，我们要想办法将每个3D keypoint的$$n$$个不同camera coordinate frame下的坐标统一起来。一个最简单的方法就是取平均（肯定是需要将每个camera frame下的坐标unproject回到统一的world coordinate system下才能取平均的）。但是在某些角度下的keypoints可能被遮挡，从而预测效果并不好。为了解决这个问题，我们利用之前的confidence maps来设计一个加权平均。这使得我们对于那些不那么自信的view里获得的keypoint的权重要小一些，从而不影响整体的效果。

我们可以为从相机$$n$$获取的3D keypoint $$k$$的坐标设置一个confidence score $$A_n^k$$，其和confidence map $$C_n^k$$的平均值成比例，而且对于$$K$$个keypoints，还做了归一化处理：

$$A_n^k = \frac{\text{exp}(\frac{1}{HW}\sum_{p=1}^W \sum_{q=1}^H C_n^k(p,q))}{\sum_{i=1}^K \text{exp}(\frac{1}{HW}\sum_{p=1}^W \Sigma_{q=1}^H C_n^i(p,q))}$$

> 这个confidence score就被理解为：对于相机$$n$$来说，其对于每个3D keypoint预测的确信度，这$$K$$个确信度的总和需要是1。


**Step3 Extracting world coordinates**

对于每个相机$$n$$和每个3D keypoint $$k$$，如果已经有了由每个encoder预测到的每个camera coordinate frame下的坐标，$$\left[ \hat u_n^k, \hat v_n^k, \hat d_n^k \right]^T$$，$$n= 1,\cdots, N$$，$$k=1, \cdots, K$$，我们现在可以将其unproject回world coordinate system：

$$\left[ \hat x_n^k, \hat y_n^k, \hat z_n^k \right]^T = \Omega_n^{-1}(\left[ \hat u_n^k, \hat v_n^k, \hat d_n^k \right]^T)$$

从而我们就得到了从相机$$n$$获取到的3D keypoint $$k$$在world coordinate system下的坐标。对于每个3D keypoint $$k$$，我们都有$$N$$个预测的结果，利用之前计算的$$A_n^k$$来计算一个加权的world coordinate：

$$\left[\bar x^k, \bar y^k, \bar z^k \right]^T = \sum_{n=1}^N \frac{A_n^k}{\sum_{m=1}^N A_n^m} \left[ \hat x_n^k, \hat y_n^k, \hat z_n^k \right]^T$$

就得到了最终的每个3D keypoint $$k$$在world coordinate system下的坐标。

**Step4 Keypoint Decoder**

我们在decoder之前，还需要将$$K$$个3D keypoints都project到每个camera coordinate frame上来增强模型的学习能力。对于相机$$n$$和3D keypoint $$k$$，我们有

$$\left[\bar u, \bar v, \bar d \right]^T = \Omega_n(\left[\bar x^k, \bar y^k, \bar z^k \right]^T)$$

对于每个相机$$n$$和每个3D keypoint $$k$$，先构建一个均值为$$\left[\bar u, \bar v \right]$$，方差为$$I_2 / \bar d$$的2维高斯分布$$G_n^k$$。记$$\bar A^k = \frac{1}{N} \sum_{n=1}^N A_n^k$$为对于所有相机$$n$$，3D keypoint $$k$$的平均confidence score。从而每个decoder $$\psi_n$$将上述构建好的高斯分布$$G_n^k$$以$$\bar A^k$$为权重，stack为一个$$G_n$$，作为输入来重构image $$I_n$$，其中

$$G_n = \text{stack}(\left[G_n^1 \bar A^1, \cdots, G_n^K \bar A^K \right])$$

最终的训练loss包括decoder得到的reconstruction image和原输入image的差别loss、由每个相机得到的每个3D keypoint $$k$$的world coordinate坐标$$\left[ \hat x_n^k, \hat y_n^k, \hat z_n^k \right]$$需要相同的loss，以及每个相机得到的3D keypoint $$k$$的world coordinate坐标需要separated的loss。


#### \[**CVPR 2022**\] [Watch It Move: Unsupervised Discovery of 3D Joints for Re-Posing of Articulated Objects](https://openaccess.thecvf.com/content/CVPR2022/html/Noguchi_Watch_It_Move_Unsupervised_Discovery_of_3D_Joints_for_Re-Posing_CVPR_2022_paper.html)


### 3D Keypoints with 3D Reconstruction

#### \[**ACCV 2022**\] [Neural Puppeteer: Keypoint-Based Neural Rendering of Dynamic Shapes](https://urs-waldmann.github.io/NePu/)

#### \[**ECCV 2022**\] [KeypointNeRF: Generalizing Image-based Volumetric Avatars using Relative Spatial Encoding of Keypoints](https://markomih.github.io/KeypointNeRF/)
