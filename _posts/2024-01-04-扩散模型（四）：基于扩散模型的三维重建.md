---
layout: post
comments: True
title: "扩散模型（四）：基于扩散模型的三维重建"
date: 2024-01-04 01:09:00
tags: diffusion; reconstruction
---

<!--more-->

{: class="table-of-content"}
* TOC
{:toc}

---

## 1. [DreamFusion](https://dreamfusion3d.github.io/) 开启了使用预训练好的2D diffusion模型获取3D representation的先河

DreamFusion原论文的全名是DreamFusion: Text-to-3D using 2D Diffusion，荣获了ICLR2023的outstanding paper award，同时也成为后续大量科研工作的baseline，其通讯作者Ben Mildenhall就是NeRF的一作，而三作Jonathan T. Barron，更是3D领域的重量级。

原论文没有给code implementation，但有大佬公布了[re-implement的GitHub仓库](https://github.com/ashawkey/stable-dreamfusion)。

![18]({{ '/assets/images/diffusion_18.png' | relative_url }}){: width=800px style="float:center"} 

文章里的这句话很精髓，直接引用：

> We are not interested in sampling pixels; we instead want to **create 3D models that look like good images when rendered from
random angles**. Such models can be specified as a differentiable image parameterization (DIP), where a differentiable generator $$g$$ transforms parameters $$\theta$$ to create an image $$x = g(\theta)$$. DIPs allow us to express constraints, optimize in more compact spaces (e.g.~arbitrary resolution coordinate-based MLPs), or leverage more powerful optimization algorithms for traversing pixel space. For 3D, we let $$\theta$$ be parameters of a 3D volume and $$g$$ a volumetric renderer. To learn these parameters, we require a loss function that can be applied to diffusion models.

DreamFusion是第一个（应该是？）利用pre-trained好的2D diffusion model来获取3D representation的论文，其想法和思路都很直接：对于一个已经预训练好的2D diffusion model，以及一个文字输入，其能够生成满足该文字语义的图片。从而我们可以初始化一个3D representation（在DreamFusion里就是一个NeRF），每次随机sample一个viewpoint，结合NeRF渲染出这个角度的图片，该图片需要与文字输入相匹配，如何利用pre-trained的diffusion model来衡量这种匹配值，是关键，DreamDiffusion提出了Score Distillation Sampling（SDS）来约束这个匹配值，下面就来具体介绍这个SDS的细节。

首先，不管是DDPM还是NCSN，其训练的loss，记为$$\mathcal{L}_{diff}$$都可以写为如下形式：

$$\mathcal{L}_{diff}(\phi) = \mathop{\mathbb{E}}\limits_{x \sim q(x), t \sim \left[2, T \right], \epsilon \sim \mathcal{N}(\textbf{0}, \textbf{I})} \left[ w(t) \lVert f_{\phi}(\alpha_t x + \sigma_t \epsilon) - \epsilon \rVert_2^2 \right]$$

也可以对于每个数据$$x \sim q(x)$$都定义一个$$\mathcal{L}_{diff}(\phi, x)$$，那么$$\mathcal{L}_{diff}(\phi) = \mathop{\mathbb{E}}\limits_{x \sim q(x)} \mathcal{L}_{diff}(\phi, x)$$。

其中$$w(t)$$是DDPM里的$$\frac{\bar{\alpha}_{t-1}\beta_t^2}{2\tilde{\beta_t} (1-\bar{\alpha}_t)^2}$$，也是NCSN里的$$\lambda_t(\sigma_t)$$。

现在我们有了一个预训练好的diffusion model（即$$\phi$$是固定的），我们想要训练的是一个3D representation（即上面所说的$$g(\theta)$$）的参数（即$$\theta$$），使得对于其在任意视角下得到的新图片$$x = g(\theta)$$，最小化$$\mathcal{L}_{diff}(\phi, x=g(\theta))$$：

$$\theta^{\ast} = \arg\min\limits_{\theta} \mathcal{L}_{diff}(\phi, x=g(\theta)) = \arg\min\limits_{\theta} \mathop{\mathbb{E}}\limits_{t \sim \left[2, T \right], \epsilon \sim \mathcal{N}(\textbf{0}, \textbf{I})} \left[ w(t) \lVert f_{\phi}(\alpha_t g(\theta) + \sigma_t \epsilon, t) - \epsilon \rVert_2^2 \right]$$

但实际上，用上述loss是无法得到一个好的3D representation结果的（在这篇文章里，$$g(\theta)$$就是NeRF）。甚至不考虑NeRF，直接让$$x$$变成被优化的目标（即$$g(\theta)$$是个identity map），按照上述loss也难以得到一个符合文本描述的图片$$x$$。文章里说有些research论文说如果仔细挑选timesteps，改变优化策略和训练策略是有可能得到好的结果的，但作者认为这样做太复杂了，而且太过于工程化，且不易推广到一般情况。

作者的做法是，换个新的loss。

首先，文章分析了为什么使用上述的loss，$$\mathcal{L}_{diff}(\phi, x=g(\theta))$$效果不好。计算该loss对$$\theta$$的导数：

$$\nabla_{\theta} \mathcal{L}_{diff}(\phi, x=g(\theta)) = \mathop{\mathbb{E}}\limits_{t \sim \left[2, T \right], \epsilon \sim \mathcal{N}(\textbf{0}, \textbf{I})} \left[ 2 \alpha_t w(t) (f_{\phi}(\alpha_t g(\theta) + \sigma_t \epsilon, t, y) - \epsilon) \frac{\partial f_{\phi}(\alpha_t g(\theta) + \sigma_t \epsilon, t, y)}{\partial (\alpha_t g(\theta) + \sigma_t \epsilon)} \frac{\partial g(\theta)}{\partial \theta} \right]$$

其中$$y$$是text embedding。最后一个表达式的期望表示式内部，有三项乘积组成（不考虑系数$$2 \alpha_t w(t)$$），其中第一项$$f_{\phi}(\alpha_t g(\theta) + \sigma_t \epsilon, t, y) - \epsilon$$叫做noise residual，第二项$$\frac{\partial f_{\phi}(\alpha_t g(\theta) + \sigma_t \epsilon, t, y)}{\alpha_t g(\theta) + \sigma_t \epsilon}$$叫做UNet Jacobbian，第三项$$\frac{\partial g(\theta)}{\theta}$$叫做generator Jacobbian。

作者发现，计算中间那项计算十分复杂（因为UNet结构复杂），并且会造成结果变差，所以不如去掉这项，从而得到了新的loss，记为$$\mathcal{L}_{SDS}(\phi, x=g(\theta))$$，这个新loss对于$$\theta$$的梯度，就是上述$$\mathcal{L}_{diff}$$对于$$\theta$$的梯度去掉了中间那项：

$$\nabla_{\theta} \mathcal{L}_{SDS}(\phi, x=g(\theta)) \triangleq \mathop{\mathbb{E}}\limits_{t \sim \left[2, T \right], \epsilon \sim \mathcal{N}(\textbf{0}, \textbf{I})} \left[ 2 \alpha_t w(t) (f_{\phi}(\alpha_t g(\theta) + \sigma_t \epsilon, t, y) - \epsilon) \frac{\partial g(\theta)}{\theta} \right]$$

有其他文章给出了下面的结果：

$$\nabla_{\theta} \mathcal{L}_{SDS}(\phi, x=g(\theta)) = \nabla_{\theta} \mathop{\mathbb{E}}\limits_{t \sim \left[2, T \right], \epsilon \sim \mathcal{N}(\textbf{0}, \textbf{I})} \left[ 2\sigma_t w(t) \textbf{D}_{\textbf{KL}}(q(\alpha_t g(\theta) + \sigma_t \epsilon; g(\theta), y, t) \Vert p_{\phi}(\alpha_t g(\theta) + \sigma_t \epsilon; y, t)) \right]$$

其中$$q(\alpha_t g(\theta) + \sigma_t \epsilon; g(\theta), y, t)$$表示的是前向扩散过程在数据$$g(\theta)$$上加噪之后（即$$\alpha_t g(\theta) + \sigma_t \epsilon$$），该噪声数据的概率分布。而该结果是知道的，是一个高斯分布，也就是DDPM里的$$q(x_t \vert x_0) = \mathcal{N}(x_t; \sqrt{\bar{\alpha}_t}x_0, (1 - \bar{\alpha}_t)\mathbf{I})$$。而$$p_{\phi}(\alpha_t g(\theta) + \sigma_t \epsilon; y, t)$$表示的是反向扩散过程，加了噪声的数据$$\alpha_t g(\theta) + \sigma_t \epsilon$$的分布，此时该分布的均值由网络预测出（输入是该噪声数据本身和时间$$t$$），同样也是一个高斯分布。

有了上述结果，就可以知道，$$\mathcal{L}_{SDS}(\phi, x=g(\theta))$$和$$\mathop{\mathbb{E}}\limits_{t \sim \left[2, T \right],  \epsilon \sim \mathcal{N}(\textbf{0}, \textbf{I})} \left[ 2\sigma_t w(t) \textbf{D}_{\textbf{KL}}(q(\alpha_t g(\theta) + \sigma_t \epsilon; g(\theta), y, t) \Vert p_{\phi}(\alpha_t g(\theta) + \sigma_t \epsilon; y, t)) \right]$$只差了一个和$$\theta$$无关的常数，从而知道了loss值，就可以观察loss变化趋势了。

由$$\mathcal{L}_{SDS}$$的表达式可以看到，其并不需要计算关于diffusion model的反向传播（只需要一个forward pass来计算$$f_{\phi}(\alpha_t g(\theta) + \sigma_t \epsilon, t, y)$$的值），所以diffusion model的作用是一个efficient, frozen critic that predicts image-space edits。

DreamFusion的流程图如下：

![19]({{ '/assets/images/diffusion_19.png' | relative_url }}){: width=800px style="float:center"} 

$$z_t$$就是我们加噪之后的数据，也就是之前公式里的$$\alpha_t g(\theta) + \sigma_t \epsilon$$，$$\hat{x}_{\phi}(z_t \vert y; t)$$是以$$z_t$$作为输入，经由diffusion models预测得到的输入数据，也就是该值需要近似$$g(\theta)$$，$$\hat{\epsilon}_t(z_t \vert y ; t)$$是diffusion models以$$z_t$$作为输入，预测得到的$$z_t$$所加上的噪声值，也就需要近似等于$$\epsilon$$。

现在已经知道了DreamDiffusion是如何使用一个预训练好的diffusion models，利用SDS loss来optimize一个NeRF的参数了。下面我们来看一下具体的实现细节。

## (1). diffusion models的选取

DreamFusion选择了一个使用图像文本对训练的可以根据输入文本生成语义一致的图片的diffusion models，叫做Imagen。其使用了basic的Imagen模型，即输出图像尺寸为$$64 \times 64$$，而且不对它做任何调整，只是拿来用。

## (2). NeRF的选取和结构细节

DreamFusion每次随即设置一个相机位置、相机旋转角度，来render一个新的view，然后按照上述所说的步骤来优化NeRF的参数。作者使用的是mip-NeRF 360（可能是因为其作者和DreamFusion的作者高度重合）。

传统的NeRF每次的输入是相机参数和一个3D坐标，输出是该点的opacity（或者叫volume density）$$\tau$$，用来衡量3D空间里该点位于物体的表面与否）以及emit radiance，也就是与相机视角相关的RGB值（opacity与相机视角无关）。而DreamFusion则是同样以相机参数和一个3D坐标为输入，输出的是该点的opacity和基于RGB的albedo（基于RGB的albedo和RGB的区别在于，其代表的是该点材料本身的颜色，与视角、光照等其他外界因素都没有关系）$$\rho$$，即：

$$(\rho, \tau) = \textbf{MLP}(\mu; \theta)$$

其中$$\mu$$是空间三维坐标，$$\theta$$是相机内参外参。

在有了每个三维点的albedo之后，还需要结合光照条件来得到最终relistic的该点的RGB值（这个过程叫做shading），而该点的shading，需要计算该点的normal vector（其表示该点的局部几何特征，即该点的normal是垂直于该点的切平面的）。而每个三维点的surface normal vector的计算如下：

$$n = -\nabla_{\mu} \tau / \lVert \nabla_{\mu} \tau \rVert$$

在有了每个点的albedo $$\rho$$，normal $$n$$，并且确定了点光源的三维位置$$l$$，点光源的颜色$$l_{\rho}$$，以及环境光颜色$$l_{a}$$之后，

每个三维点$$\mu$$的最终的color就可以计算出来了：

$$c = \rho (l_{\rho} \max(0, n \cdot (l-\mu) / \Vert l-mu \Vert) + l_a)$$

有了每个点的color $$c$$和opacity $$\tau$$之后，就可以像classical NeRF一样render图片了。

作者还发现，随机的将某些点的albedo直接替换成白色的，即$$(1,1,1)$$，能够更好的学习场景的texture。这还可以防止模型学到flat的geometry：比如说场景里有一副画了松鼠的画，以及有一个真实的三维松鼠，在某些很多相机角度下都能render出类似的图片。

作者还发现，需要限制NeRF的三维采样点的范围在一个球面内，并且利用另一个MLP来建模环境光，其输入是每个三维点相对于相机的ray direction，输出是这个点的环境光。


## 2. 使用pre-trained 2D diffusion models的text-to-3D论文

CVPR2024的[Text-to-3D using Gaussian Splatting]()，CVPR2024的[GaussianDreamer]()，CVPR2024的[RichDreamer](https://aigc3d.github.io/richdreamer/)，ILCR2024的[MVDream](https://github.com/bytedance/MVDream/tree/main)

### (1). MVDream: Multi-view diffusion for 3D generation

[CODE](https://github.com/bytedance/MVDream/tree/main)是有的。


## 3. 使用pre-trained 2D diffusion models的image-to-3D论文

ICCV2023的[Zero-1-to-3]()，ICLR2024Spotlight的[SyncDreamer]()，CVPR2024Highlight的[Wonder3d]()，CVPR2023的[DreamBooth3D]()，ICLR2024的[Magic123](https://guochengqian.github.io/project/magic123/)，CVPR2024的[The More You See in 2D, the More You Perceive in 3D](https://sap3d.github.io/)，ECCV2024的[3DCongealing]()，NeurIPS2023的[One-2-3-45](https://github.com/One-2-3-45/One-2-3-45/tree/master)，NeurIPS2024 Oral的[CAT3D](https://cat3d.github.io/)

### (1). Zero-1-to-3: Zero-shot One Image to 3D Object

代码[code](https://github.com/cvlab-columbia/zero123)是available的。

这篇ICCV2023年的论文的主要贡献就在于enable之前的pre-trained的diffusion model根据输入的图片和relative pose $$(R, T)$$来生成和这张图片的relative pose正好是$$(R,T)$$的新角度的图片。也就是enable diffusion模型根据camera pose生成viewpoint-conditioned的图片。

这段话是本文的核心思想：

> Since diffusion models have been trained on internet-scale data, their support of the natural image distribution likely covers most viewpoints for most objects, but these viewpoints cannot be controlled in the pre-trained models. Once we are able to teach the model a mechanism to control the camera extrinsics with which a photo is captured, then we unlock the ability to perform novel view synthesis.

### (2). 3D Congealing: 3D-Aware Image Alignment in the Wild

3DCongealing的输入是一系列category-specific的2D RGB图片，目标是将这些图片align到一起，和之前那些2D congealing的目标相同（比如[neuralcongealing]()，[GANgealing]()，[ASIC]()等）。但不同的是，3DCongealing的输入图片可以有很大的姿态差别（相机角度差别），比如说即使都是马的图片，但一张是马的侧面，一张是马的背面。那这种情况下，如果还只是做2D congealing，也就是将一张图片上的像素点和另一张图片的像素点相匹配上，就没有现实意义了（也做不了了），因为两张图片的common的semantically consistent的像素点甚至不存在了。但我们知道，它们都是描述同一类3D物体的，所以说我们可以考虑将这些2D图片，congeal或者说align到一个3D的shape上去。这就是3DCongealing这篇文章要做的。

输入仅仅是2D RGB图片，没有任何别的标注信息，这个任务是很难的。但有pre-trained的2D diffusion model，就有办法。不同于那些基于pre-trained 2D diffusion models以图片为condition来optimize一个3D representation的方法（即image-to-3D），3DCongealing对于输入的$$N$$张图片，先用[Textual Inversion](https://github.com/rinongal/textual_inversion)生成一个文本（或者文本embedding）来collaboratively描述这些图片，这个文本记为$$y^{\ast}$$。然后再利用DreamFusion里的方法，基于$$y^{\ast}$$，来optimize一个NeRF来表示3D shape。作者说那些image-to-3D的办法（比如DreamBooth3D）是利用图片来finetune预训练好的diffusion model，训练时间长，更复杂。

除了上述的optimize NeRF的loss，作者还提出了一个alignment loss。也就是对于每张输入的图片，用一个网络来预测其camera pose，再利用该camera pose和那个NeRF渲染得到这个角度的图片，再在DINO feature space上，计算这个渲染的图片和原输入图片之间的差距。

这两个loss共同作为训练网络的指标。但在implementation details里，作者说需要先将后一个loss的权重设置为0，先optimize一个NeRF出来，然后fix它了，之后再optimize camera poses。而且在optimize camera poses的时候，需要进行多次采样否则难以训练得到好的结果。这样的操作实际上是multi-stage的，减小了难度，但也降低了效果。

![20]({{ '/assets/images/diffusion_20.png' | relative_url }})
{: style="width: 1200px; max-width: 100%;"}


## 4. 使用pre-trained 2D diffusion models实现3D edit/animation/deformation的论文

CVPR2024的[AlignYourGaussians]()，CVPR2024的[GaussianEditor]()，SiggraphAsia2023的[Dreameditor]()，[Animate124](https://animate124.github.io/)，NeurIPS2023的[ViCA-NeRF]()，CVPR2024年的[As-Plausible-As-Possible](https://as-plausible-as-possible.github.io/)，CVPR2024的[Dream-in-4D](https://github.com/NVlabs/dream-in-4d)，TOG2024的[TIP-Editor](https://zjy526223908.github.io/TIP-Editor/)，[GenXD](https://gen-x-d.github.io/)
