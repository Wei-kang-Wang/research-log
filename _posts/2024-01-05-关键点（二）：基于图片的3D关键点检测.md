---
layout: post
comments: True
title: "关键点（二）：基于图片的3D关键点检测"
date: 2024-01-05 02:09:00
tags: keypoint
---

<!--more-->

{: class="table-of-content"}
* TOC
{:toc}

---


## Supervised 3D Keypoint Detection

### \[**ICCV 2017**\] [How far are we from solving the 2D & 3D Face Alignment problem?](https://github.com/1adrianb/2D-and-3D-face-alignment)

[CODE1](https://github.com/1adrianb/2D-and-3D-face-alignment), [CODE2](https://github.com/1adrianb/face-alignment)

### \[**NeurIPS 2018 Oral**\] [Occlusion-Net: 2D/3D Occluded Keypoint Localization Using Graph Networks](https://openaccess.thecvf.com/content_CVPR_2019/papers/Reddy_Occlusion-Net_2D3D_Occluded_Keypoint_Localization_Using_Graph_Networks_CVPR_2019_paper.pdf)

[CODE](https://github.com/dineshreddy91/Occlusion_Net)

![unsuper1]({{ '/assets/images/occlusion_1.png' | relative_url }}){: width=800px style="float:center"} 

这篇文章的输入是RGB图片，标注信息包括$$K$$个keypoint的2D image coordinate，以及这$$K$$个keypoint的visibility的情况（0或者1），并且这$$K$$个keypoints的顺序需要确保（是有序序列，等价于keypoints有labels）。

> 也就是说，本文的输入setting和C3DPO等NrSfM方法的setting是一样的

本文的总体思路是，使用Graph Neural Network来对所有的$$K$$个keypoint以及它们之间的edges进行建模，从而可以用每张图片的visible keypoints来预测invisible keypoints。keypoints之间的edges就是0或者1（如果两个keypoints在该张图片都可见，就是1，其他情况是0）。

具体来说，本文使用的是监督学习的方法，框架包括三个部分：2DKNN Encoder，2DKNN Decoder以及3DKNN Encoder。

首先，使用Mask-RCNN从输入图片中输出$$K$$个heatmap来获得$$K$$个keypoints的坐标$$(x_i, y_i), i=1,2,\cdots,K$$，同时也使用heatmaps来获得每个keypoint的confidence $$c_i,i=1,2,\cdots,K$$，再加上该keypoints的ground truth visibility $$t_i, i=1,2,\cdots,K$$，共同作为GNN的每个node的输入features $$\mathcal{V}_i = (x_i,y_i,c_i,t_i)$$，输入给2DKNN Encoder来使用GNN得到edge的prediction。此处有两个losses：由heatmaps得到的keypoints坐标和ground truth坐标之间的差距（只考虑visibility ground truth可见的那些点），预测得到的edges和ground truth edges之间的差距（ground truth edges由ground truth visibility计算得到）。

之后，将由2DKNN Encoder更新后的graph输入给2DKNN Decoder，来进一步优化$$K$$个keypoints的坐标位置。但是这一步的loss仅计算ground truth occluded的keypoints（即visibility ground truth是0的那些点）和预测keypoints之间的差异，而且该差异是通过对于每张照片，使用其同一个场景另外两个views，利用每个keypoint在这三张照片里的trifocal tensor的trilinear constraints来计算的loss。

最后，再将由2DKNN Decoder更新后的graph输入给3DKNN Encoder，输出basis和camera pose estimation $$Pi$$，和C3DPO相同，用bases表示3D keypoints，然后使用$$Pi$$将其project到2D相机平面上，最后计算projected的3D keypoints和ground truth的2D keypoints之间的差距（注意是所有的keypoints，不考虑可不可见，都算上）。

最后将这四个losses作为最终的训练loss来同时优化这三个部分。

> 本文的实验是在一个car dataset上做的，该dataset背景复杂，且可能含有多个cars，但car本身是rigid objects，所以该数据集有它的难度，但在rigidity上变化较小。而且具体操作的时候，每次是将每张照片按照ground truth的bounding box裁剪掉多余的背景之后再做为输入，也就是每次只有一个center positioned的car，如上面流程图所示。

![unsuper1]({{ '/assets/images/occlusion_2.png' | relative_url }}){: width=800px style="float:center"} 


### \[**ICLR 2021**\] [Semi-supervised Keypoint Localization](https://olgamoskvyak.github.io/files/paper5.pdf)

[CODE](https://github.com/olgamoskvyak/tf_equivariance_loss)


### \[**ICLR 2022**\] [Pseudo-labeled Auto-curriculum Learning for Semi-supervised Keypoint Localization](https://openreview.net/pdf?id=6Q52pZ-Th7N)




### \[**CVPR 2023**\] [Few-shot Geometry-Aware Keypoint Localization](https://xingzhehe.github.io/FewShot3DKP/)

这篇文章提出的是一种semi-supervised的方法来学习2D keypoints，其输入为某个类别的图片集合，其中一小部分图片标注有ground truth的$$K$$个keypoints的annotations，但不论是否visible，都标记上了（也就是没有visibility的标注），而且还有物体的keypoints edges linkage的信息，以及物体的keypoints如何被分为几个subparts的信息。

![unsuper1]({{ '/assets/images/fewshot_1.png' | relative_url }}){: width=800px style="float:center"} 

该方法的框架是基于AutoLink的，其使用$$K$$个heatmaps来获得$$K$$个keypoints，然后根据keypoints来构造edge map（每对keypoints构成的edge map含有一个可学习的系数），并且将原图片randomly masked的features与edge map结合输入给decoder来reconstruct原输入图片，并且设计一个reconstruction loss，这些都与AutoLink相同。不同的是，类似于[Unsupervised learning of object landmarks by factorized spatial embeddings](https://openaccess.thecvf.com/content_ICCV_2017/papers/Thewlis_Unsupervised_Learning_of_ICCV_2017_paper.pdf)，本文也使用了equivariance loss。并且网络不仅输出了$$K$$个heatmaps，还输出了$$K$$个uncertainty maps $$V_i \in \left(0, 1 \right)^{H \times W}, \ i=1,2,\cdots,K$$，并且最终的每个keypoint的uncertainty由经过softmax的heatmap $$H_i$$与uncertainty map $$V_i$$进行element-wise加权求和而来。这个uncertainty $$v_i$$被用在edge map的计算里。并且，网络还对于每个keypoint都输出了一个dense的depth map $$D_i \in \mathbb{R}^{H \times W}$$，每个keypoint的depth也是由$$D_i$$和对应的heatmap的softmax加权平均而来，每个3D keypoints的坐标就是由heatmap得到的2D坐标扩展为3维（扩展的维度的值就是该点的depth）。而对这些3D keypoints的约束则是对于每个3D keypoints set，大小为$$K \times 3$$，先按照物体预定义好的subparts，将其分为$$\mathcal{P}$$个部分，对于每个部分，对于每对随机选择的两张图片$$\pmb{I}, \pmb{I}^{'}$$，有两组该部分的3D keypoints，$$P_{\pmb{I}}^j \in \mathbb{R}^{N_j \times 3}$$，$$P_{\pmb{I}^{'}}^j \in \mathbb{R}^{N_j \times 3}$$，$$j=1,2,\cdots,\mathcal{P}$$，其中$$N_j$$表示第$$j$$个部分点的数量。计算$$P_{\pmb{I}}^j$$和$$P_{\pmb{I}^{'}}^j$$之间的procrustes analysis，然后将aligned之后的$$P_{\pmb{I}}^j$$和$$P_{\pmb{I}^{'}}^j$$之间的距离，按照$$\mathcal{P}$$个部分求平均，称为3D geometry loss。最后，对于那些有标注的图片，还有个supervised的loss。

![unsuper1]({{ '/assets/images/fewshot_2.png' | relative_url }}){: width=800px style="float:center"} 

### \[**CVPR 2024**\] [3D-LFM: Lifting Foundation Model](https://3dlfm.github.io/)


## Unsupervised 3D Keypoint Detection

### \[**ECCV 2018**\] [Unsupervised Domain Adaptation for 3D Keypoint Estimation via View Consistency](https://github.com/xingyizhou/3DKeypoints-DA)

[CODE](https://github.com/xingyizhou/3DKeypoints-DA)

这篇文章的想法和那篇unsupervised 2D keypoint detection的论文[Object landmark discovery through unsupervised adaptation](https://proceedings.neurips.cc/paper/2019/hash/97c99dd2a042908aabc0bafc64ddc028-Abstract.html)类似，都是让网络在某个类别的source datasets上supervised训练，然后将这个网络参数微调，使得其能够适应新的另一个类别的target dataset。但这里的数据是3D keypoints。

这篇文章的数据集包含了丰富的信息。数据集构成为：一个source datasets，每个数据点都有RGB图片$$\pmb{x}$$以及对应的3D keypoint matrix $$P \in \mathbb{R}^{K \times 3}$$；一个target datasets，其包含$$N$$个场景，而每个场景又包含$$M_j$$个multi views，$$j=1,2,\cdots, N$$。

具体做法是，使用一个神经网络$$\Phi$$来使用图片作为输入，输出3D keypoint matrix，损失函数包括三个：（1）在source datasets上预测的3D keypoints和ground truth之间的差异；（2）在target dataset上，对于每个场景，同时也optimize一个$$M_i \in \mathbb{R}^{K \times 3}$$，代表这个场景在canonical frame下的3D keypoints的坐标，然后对于这个场景的每个view $$I_j, j=1,2,\cdots,M_j$$，先得到3D keypoints的输出$$X_{I_j} = \Phi(I_j) \in \mathbb{R}^{K \times 3}$$，然后使用procrustes analysis将$$X_{I_j}$$align到$$M_i$$上去，计算aligned之后的$$X_{I_j}$$和$$M_i$$之间的距离，即为multi-view loss；（3）最后一个loss用来减小网络对于source dataset和target dataset输出之间的差异，具体做法是，对于每个$$M_i$$，其对source datasets里的每个ground truth $$Y_j$$都做procrustes analysis，然后计算aligned后的$$M_i$$和这些$$Y_j$$之间的距离，选择最小的那个作为该$$M_i$$和source datasets之间的距离，然后对所有的$$M_i$$做上述操作，将距离求和，即为chamfer loss。

> 该方法直接利用神经网络以图片为输入，每次都输出所有的3D keypoints，没有考虑visibility

### \[**NeurIPS 2018 Oral**\] [Discovery of Latent 3D Keypoints via End-to-end Geometric Reasoning](https://keypointnet.github.io/keypointnet_neurips.pdf)

[Review](https://papers.nips.cc/paper/2018/file/24146db4eb48c718b84cae0a0799dcfc-Reviews.html)
[CODE1](https://github.com/ZhengyiLuo/KeypointNet), [CODE2](https://github.com/ihankyang/keypoint-network), [CODE3](https://github.com/Noone65536/KeypointNet_network)

对于使用无监督的方式来获取keypoints，本文的思路是将这些keypoints要用在什么下游任务这个信息考虑进来。利用下游任务来优化keypoints的选择会自然的鼓励网络学到对于这个下游任务最有用的那些keypoints。这篇文章提出的KeypointNet是一个end-to-end的geometric reasoning框架，对于某个特别的下游任务，学习一个最优的category-specific以及task-specific 3D keypoints集合。该方法的创新点在于我们引入一个任意的下游任务来将3D keypoints作为latent variable学习，从而做到无监督。该方法对于任何目标函数相对于3D keypoints positions是可微分的下游任务都是可以使用的。

本文使用了3D pose estimation作为下游任务，即目标函数旨在为恢复同一个object的两个views之间的relative pose。

> 3D pose estimation的标准做法是：1）先检测一个稀疏的category-specific keypoints的集合；2）之后再利用这些keypoints使用某种geometric reasoning框架（比如说，PnP algorithm）来重建3D pose或者相机角度。

![PIPELINE]({{ '/assets/images/DOWNSTREAM-2.PNG' | relative_url }}){: width=800px style="float:center"}
*在训练过程中，同一个object的两个views都会给KeypointNet作为输入。两个views之间的rigid transformation $$(R,t)$$也作为监督信号被提供。我们学习一个3D keypoints的ordered list，在两个views里是consistent的，而且能够让我们复原transformation。在inference的时候，KeypointNet从一张单独的输入图片提取3D keypoint positions*

训练集的输入包括一对图片$$(I,I^{'})$$，其是同一个object的两个不同角度的图片，以及它们的relative rigid transformation $$T \in SE(3)$$。我们将通过优化一个目标函数$$O(f_{\theta}(I), f_{\theta}(I^{'}))$$，来学习一个函数$$f_{\theta}(I)$$，参数是$$\theta$$，将一个2D的图片$$I$$映射到一个3D keypoint locations的list，$$P=(p_1,p_2,...,p_N)$$，其中$$p_i=(u_i,v_i,z_i)$$。

函数$$f_{\theta}(I)$$由神经网络来实现，其输出是$$N$$个heatmap，$$g_i(u,v)$$，表示的是keypoint i在pixel位置$$(u,v)$$地方可能出现的概率，从而$$\left[ u_i, v_i \right]^T = \sum_{u,v} \left[ ug_i(u,v), vg_i(u,v) \right]^T$$

对于$$z$$ coordinates来说，网络$$f_{\theta}(I)$$还输出了$$N$$个深度的heatmap，$$d_i(u,v)$$，从而$$z_i = \sum_{u,v} d_i(u,v) g_i(u,v)$$

该方法使用了两个loss：（1）用来衡量两个views的3D keypoints matrix的multi-view consistency loss，其目的是确保3D keypoints在不同的views下仍然consistent，即两个views里的同一个3D keypoint应该在3D空间里对应到真实object的同一个点；（2）由下游代理任务引入的loss，在本文里是3D pose estimation loss

对于multi-view consistency loss来说，假设已知相机的global focal length $$f$$，使用$$\left[ x,y,z \right]$$代表world coordinate，使用$$\left[ u,v \right]$$代表image coordinate，并且使用perspective projection来作为2D-3D transformation。假设从$$f_{\theta}(I)$$输出所得到的第$$i$$个3D keypoint的inhomogeneous coordinate是$$\left[ x_i, y_i, z_i, 1 \right]$$，那么该点在$$I$$上的2D投影为：$$\left[u_i, v_i, 1 \right] = \left[ fx/z, fy/z, 1 \right]$$。假设从$$f_{\theta}(I^{\prime})$$输出所得到的第$$i$$个3D keypoint的inhomogeneous coordinate是$$\left[ x_i^{\prime}, y_i^{\prime}, z_i^{\prime}, 1 \right]$$，而$$I$$和$$I^{\prime}$$之间ground truth的transformation是$$T \in SE(3)$$，从而该点经过transformation $$T$$之后（为$$T \left[ x_i, y_i, z_i, 1 \right]^T$$）在$$I$$上的2D投影记为：$$\left[\hat u_i^{\prime}, \hat v_i^{\prime}, 1 \right]$$。同理定义$$\left[u_i^{\prime}, v_i^{\prime}, 1 \right]$$和$$\left[\hat u_i, \hat v_i, 1 \right]$$。从而symmetric multi-view consistency loss定义为：

$$\mathcal{L}_{con} = \frac{1}{2N}\sum_{i=1}^N \lVert \left[ u_i, v_i, u_i^{\prime}, v_i^{\prime} \right]^T - \left[ \hat u_{i}^{\prime}, \hat v_{i}^{\prime}, \hat u_{i}, \hat v_{i} \right]^T \rVert^2$$

3D pose estimation loss用来衡量预测的relative rotation $$\hat{R}$$（用两个3D keypoints matrix利用Procrustes'alignment计算得来）和ground truth rotation $$R$$之间的差异：

$$\mathcal{L}_{pose} = 2arcsin(\frac{1}{2\sqrt{2}} \lVert \hat{R}-R \rVert)$$

> 作者说，KeypointNet具有translation equivariance性质，并且已经有了上述的multi-view consistency loss，所以就不对translation的真实值和预测值进行约束

使用Procrustes'alignment来估计$$\hat{R}$$的方法如下。记$$X$$和$$X^{\prime} \in R^{3 \times N}$$为两个views在camera coordinates system下的3D keypoint locations。也就是说，$$X = \left[X_1,X_2,...,X_N \right]$$。$$\tilde{X}$$和$$\tilde{X}^{\prime}$$表示$$X$$和$$X^{\prime}$$分别将各自的mean（大小为$$3$$）减掉之后的矩阵。$$\hat{R}$$就用以下的方式计算而得：

$$\hat{R} = Vdiag(1,1,...,det(VU^T))U^T$$

其中$$U, \Sigma, V^T = SVD(\tilde X \tilde X^{\prime T})$$。

> 为了确保$$\tilde{X}\tilde{X}^{\prime T}$$是invertible的，且为了增强3D keypoints的鲁棒性，作者还在$$X$$和$$X^{\prime}$$内加入了Gaussian噪声。

> 上述计算rotation $$\hat{R}$$的问题框架叫做orthogonal Procrustes problem

> 对于很多下游的任务来说，还有很多常见的keypoints的性质可以起到帮助的作用，比如说（1）Separation loss可以对两个靠的太近的3D keypoints进行惩罚：$$\mathcal{L}_{sep} = \frac{1}{N^2}\sum_{i=1}^{N}\sum_{j \neq i}^N \max(0, \delta^2- \lVert X_i- X_j \rVert^2)$$；（2）Silhouette consistency鼓励keypoints落在object的轮廓内。我们网络对于第$$i$$个keypoint的position的预测是通过计算$$g_i$$（也就是heatmap）的期望得来的。一个确保silhouette consistency的方法就是只在silhouette内让概率非零，记图片的mask为$$b(u,v) \in \lbrace 0,1 \rbrace$$。从而$$\mathcal{L}_{obj} = \frac{1}{N}\sum_{i=1}^N -log\sum_{u,v}b(u,v)g_i(u,v)$$；（3）使得表示2D keypoint位置的distribution峰比较尖：$$\mathcal{L}_{var} = \frac{1}{N}\sum_{i=1}^N \sum_{u,v} g_i(u,v) \lVert \left[ u,v \right] - \left[ u_i, v_i \right] \rVert^2$$，



### \[**ICML 2021**\] [Unsupervised Learning of Visual 3D Keypoints for Control](http://proceedings.mlr.press/v139/chen21b/chen21b.pdf)

[Code](https://github.com/buoyancy99/unsup-3d-keypoints) [Post](https://buoyancy99.github.io/unsup-3d-keypoints/)

这篇文章的假设前提是：有$$N$$个已经标定好的相机（也就是extrinsic和intrinsic都是知道的）来产生统一个场景的$$N$$个views，然后从这$$N$$个views来无监督的获取3D keypoints

> 但实际上相机内外参给定的情况下，很多信息都给了

给定同一个场景的$$N$$个view，我们为每个view都给一个encoder和一个decoder。

$$I_n \in \mathbb{R}^{H \times W \times C}$$表示相机$$n$$的输入image，$$n \in 1,\cdots, N$$，记相机$$n$$的extrinsic matrix为$$V_n$$，intrinsic matrix为$$P_n$$。$$K$$是keypoints的个数。对于一个点的world coordinate system下的坐标$$\left[ x,y,z \right]^T$$和相机$$n$$，我们可以使用extrinsic matrix $$V_n$$和intrinsic matrix $$P_n$$来将其投射到camera coordinate system下的坐标$$\left[ u, v, d \right]^T$$，其中$$u, v \in \left[ 0,1 \right]$$是camera plane上归一化后的coordinate（这里将camera coordinate system和image coordinate system结合在一起了），$$d>0$$是depth value，也就是该点距离camera plane的距离。将上述的这种投射记为，operator $$\Omega_n: \left[x,y,z\right]^T \rightarrow \left[u,v,d\right]^T$$，而其inverse记为$$\Omega_n^{-1}$$。$$\Omega_n, \Omega_n^{-1}$$都是differentiable的。

![Model Structure]({{ '/assets/images/CONTROL-1.PNG' | relative_url }}){: width=800px style="float:center"}
*Fig 1. Overview of our Keypoint3D algorithm. (a) 对于每个camera view，一个CNN将输入image编码为$$K$$个heatmaps以及depth maps；(b) 我们将这些heatmaps当作概率来计算camera plane下keypoint横纵坐标的。我们同时也用heatmap和depth map来计算每个keypoint的深度d，这些$$\left[u,v,d\right]$$再被反投射回到world coordinate里；(c) 我们利用之前的heatmaps来计算每个camera view对于每个keypoint的置信概率，然后对于每个keypoint，我们计算出一个加权的world coordinate；(d) 我们再将每个keypoint的world coordinate投射到每个camera plane上；(e) 从而对于每个camera plane，都有$$K$$个这样的投射，建立$$K$$个高斯map，将它们叠起来，作为decoder的输入，来重构原输入图片；(f) 除了上述的这些loss，我们还将所学习到的3D keypoint的world coordinate与下游任务相结合，来共同优化这个网络。*

具体来说，分以下四步来实现：

**Step1 Keypoint Encoder**

对于每个camera $$n$$，我们将$$I_n$$喂给一个fully convolutional encoder $$\phi_n$$来获得$$k$$个confidence maps，$$C_n^k \in \mathbb{R}^{W \times H}$$，以及depth maps，$$D_n^k \in \mathbb{R}^{W \times H}$$。对于每个confidence map，我们使用一个spatial softmax来计算得到probability heatmap $$H_n^k(i,j) = \frac{\text{exp}(C_n^k(i,j)}{\sum_{p=1}^W \sum_{q=1}^H \text{exp}(C_n^k(p,q))}$$

每个3D keypoint $$k$$在相机$$n$$的camera coordinate system下的坐标即为：

$$\left[\hat u_n^k, \hat v_n^k, \hat d_n^k \right] = \left[ \frac{1}{W} \sum_{u,v} u H_n^k(u,v), \frac{1}{H} \sum_{u,v} v H_n^k(u,v), \sum_{u,v} u D_n^k(u,v) H_n^k(u,v) \rigjt]$$

> 注意到，对于$$u,v$$的计算都除以了图片尺寸，是因为计算的是归一化之后的相机坐标下的3D keypoint坐标。


**Step2 Attention**

在预测了每个camera coordinate frame下每个3D keypoint的坐标之后，我们要想办法将每个3D keypoint的$$n$$个不同camera coordinate frame下的坐标统一起来。一个最简单的方法就是取平均（肯定是需要将每个camera frame下的坐标unproject回到统一的world coordinate system下才能取平均的）。但是在某些角度下的keypoints可能被遮挡，从而预测效果并不好。为了解决这个问题，我们利用之前的confidence maps来设计一个加权平均。这使得我们对于那些不那么自信的view里获得的keypoint的权重要小一些，从而不影响整体的效果。

我们可以为从相机$$n$$获取的3D keypoint $$k$$的坐标设置一个confidence score $$A_n^k$$，其和confidence map $$C_n^k$$的平均值成比例，而且对于$$K$$个keypoints，还做了归一化处理：

$$A_n^k = \frac{\text{exp}(\frac{1}{HW}\sum_{p=1}^W \sum_{q=1}^H C_n^k(p,q))}{\sum_{i=1}^K \text{exp}(\frac{1}{HW}\sum_{p=1}^W \Sigma_{q=1}^H C_n^i(p,q))}$$

> 这个confidence score就被理解为：对于相机$$n$$来说，其对于每个3D keypoint预测的确信度，这$$K$$个确信度的总和需要是1。


**Step3 Extracting world coordinates**

对于每个相机$$n$$和每个3D keypoint $$k$$，如果已经有了由每个encoder预测到的每个camera coordinate frame下的坐标，$$\left[ \hat u_n^k, \hat v_n^k, \hat d_n^k \right]^T$$，$$n= 1,\cdots, N$$，$$k=1, \cdots, K$$，我们现在可以将其unproject回world coordinate system：

$$\left[ \hat x_n^k, \hat y_n^k, \hat z_n^k \right]^T = \Omega_n^{-1}(\left[ \hat u_n^k, \hat v_n^k, \hat d_n^k \right]^T)$$

从而我们就得到了从相机$$n$$获取到的3D keypoint $$k$$在world coordinate system下的坐标。对于每个3D keypoint $$k$$，我们都有$$N$$个预测的结果，利用之前计算的$$A_n^k$$来计算一个加权的world coordinate：

$$\left[\bar x^k, \bar y^k, \bar z^k \right]^T = \sum_{n=1}^N \frac{A_n^k}{\sum_{m=1}^N A_n^m} \left[ \hat x_n^k, \hat y_n^k, \hat z_n^k \right]^T$$

就得到了最终的每个3D keypoint $$k$$在world coordinate system下的坐标。

**Step4 Keypoint Decoder**

我们在decoder之前，还需要将$$K$$个3D keypoints都project到每个camera coordinate frame上来增强模型的学习能力。对于相机$$n$$和3D keypoint $$k$$，我们有

$$\left[\bar u, \bar v, \bar d \right]^T = \Omega_n(\left[\bar x^k, \bar y^k, \bar z^k \right]^T)$$

对于每个相机$$n$$和每个3D keypoint $$k$$，先构建一个均值为$$\left[\bar u, \bar v \right]$$，方差为$$I_2 / \bar d$$的2维高斯分布$$G_n^k$$。记$$\bar A^k = \frac{1}{N} \sum_{n=1}^N A_n^k$$为对于所有相机$$n$$，3D keypoint $$k$$的平均confidence score。从而每个decoder $$\psi_n$$将上述构建好的高斯分布$$G_n^k$$以$$\bar A^k$$为权重，stack为一个$$G_n$$，作为输入来重构image $$I_n$$，其中

$$G_n = \text{stack}(\left[G_n^1 \bar A^1, \cdots, G_n^K \bar A^K \right])$$

最终的训练loss包括decoder得到的reconstruction image和原输入image的差别loss、由每个相机得到的每个3D keypoint $$k$$的world coordinate坐标$$\left[ \hat x_n^k, \hat y_n^k, \hat z_n^k \right]$$需要相同的loss，以及每个相机得到的3D keypoint $$k$$的world coordinate坐标需要separated的loss。


### \[**CVPR 2022**\] [Watch It Move: Unsupervised Discovery of 3D Joints for Re-Posing of Articulated Objects](https://openaccess.thecvf.com/content/CVPR2022/html/Noguchi_Watch_It_Move_Unsupervised_Discovery_of_3D_Joints_for_Re-Posing_CVPR_2022_paper.html)


## 3D Keypoints with 3D Reconstruction

### \[**ACCV 2022**\] [Neural Puppeteer: Keypoint-Based Neural Rendering of Dynamic Shapes](https://urs-waldmann.github.io/NePu/)

### \[**ECCV 2022**\] [KeypointNeRF: Generalizing Image-based Volumetric Avatars using Relative Spatial Encoding of Keypoints](https://markomih.github.io/KeypointNeRF/)
