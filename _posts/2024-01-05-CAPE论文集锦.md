---
layout: post
comments: True
title: "CAPE论文集锦"
date: 2024-01-05 04:09:00

---

<!--more-->

{: class="table-of-content"}
* TOC
{:toc}

---

CAPE是category-agnostic pose estimation的简称，其仍然属于2D pose estimation问题的范畴（即2D keypoints estimation），但和category-specific pose estimation问题不同的地方在于，其致力于仅仅使用某些类别的图片进行训练，而可以在新类别的图片上具有良好的泛化效果。具体来说，训练数据是一对输入$$(\lbrace P_i^S \rbrace_{i=1}^N, P^Q$$，其中$$P_i^S, P^Q \in \mathbb{R}^{K \times 2}$$分别是support images以及query image的ground truth的2D点坐标，其中support images有$$N$$张（$$N$$可以是1）。在测试的时候，给定一个新的类别的support图片的keypoints的标注，让模型在该新类别的另一张query图片上输出keypoints的坐标值。

> 需要注意的是，不同类别的物体，keypoints数量可能不同，模型设计也需要考虑这一点（目前的方法就是直接将模型预测的keypoints数量设置为100个（因为数据集里所有类别的keypoints数量都小于100），对于每个类别，多余的部分就是dummy outputs）

CAPE问题首先在ECCV 2022 Oral的[Pose for Everything: Towards Category-Agnostic Pose Estimation](https://github.com/luminxu/Pose-for-Everything)里提出，该论文还提出了一个数据集：mp-100，也是之后所有following works测试的benchmark。

## Image-based methods

Image-based方法指的是基于图片features的方法，其仅仅使用image features作为输入数据，且仅仅使用2d keypoints locations的ground truth作为监督信息。

### \[**ECCV 2022 Oral**\] [Pose for Everything: Towards Category-Agnostic Pose Estimation](https://github.com/luminxu/Pose-for-Everything)

POMNet是由提出CAPE的论文[Pose for Everything: Towards Category-Agnostic Pose Estimation](https://github.com/luminxu/Pose-for-Everything)同时提出的框架，其思想直截了当，模型分为两个部分，首先使用某种feature extractor获取support图片2d keypoints的features和query图片pixel的features（实际操作的时候会对图片resolution降低，所以实际上就是patch-wise的features）。然后使用一个基于DETR的transformer框架来对support图片的2d keypoints features进行更新（让其获取query图片里的信息，因为support图片和query图片虽然都是同一类物体的图片，但可能会有appearance（光照、颜色、纹理等）以及geometry（姿态、articulation、deformation等）的区别）。最后使用一个网络以refined之后的support图片的2d keypoints features和query图片的features作为输入，输出query图片的2d keypoints坐标值。整个框架使用query图片的2d keypoints ground truth进行监督训练，损失函数也只有预测的keypoints location和gt keypoints location之间的mse这一项。

### \[**CVPR 2023 Highlight**\] [Matching Is Not Enough: A Two-Stage Framework for Category-Agnostic Pose Estimation](https://github.com/flyinglynx/CapeFormer)

CapeFormer于CVPR 2023 Hightlight论文[Matching Is Not Enough: A Two-Stage Framework for Category-Agnostic Pose Estimation](https://github.com/flyinglynx/CapeFormer)里提出，其基本想法是，POMNet使用一个网络来implicitly输出query图片的2d keypoints locations，Capeformer将query keypoints的输出改为explicit的，并且在其基础上增加了一步。具体来说，在使用feature extractor获取support和query图片特征，并且使用transformer来refine这些features之后，对于每个support keypoints，其和query图片的features可以计算一个similarity map，该map就可以理解为query图片该keypoint的预测heatmap。从每个similarity map可以获取keypoints的2d坐标，将其再和support keypoints features，以及query图片features一起输入给另一个模型来预测offset。最后的query图片的2d keypoints的预测值就是由similarity map获取的2d坐标加上offset之后的值。整个框架同样仅仅使用query图片的2d keypoints ground truth进行监督训练，损失函数有两个，首先是similarity map和gt的heatmap之间的差距，加上由similarity map获取的2d locations和gt locations之间的差距，以及由similarity map获取的2d locations加上offset之后和gt locations之间的差距。

> 注意的是，query图片的features不仅包括图片本身的features，还要concatenate上每个pixel或者patch所在位置的positional encoding。CapeFormer里还新增了一个给support keypoints也添加positional encoding的模块，但后续论文（GraphCape证明该模块实际上会减弱模型的泛化能力）

#### CapeFormer代码

CapeFormer的代码框架被后续一系列论文所使用，下面简单介绍一下该代码框架的主要结构和内容。

**1. 主体框架**

CapeFormer代码基于MMLab的`mmcv`和`mmpose`库。

其首先提供一系列configuration文件，在`configs/mp100/capeformer`下，分为`1shot`，`5shots`和`cross_category`三种，分别对应论文里的三个主要实验。每个configuration文件包括了数据集定义、数据集预处理、模型定义、优化器定义以及有关训练的各种超参数定义。

数据集相关代码位于`capeformer/datasets`文件夹下，其包含两个`py`文件：`__init__.py`和`build.py`，前者用于将该文件夹下有用的类和函数都import到当前文件夹下，从而可以直接使用`from capeformer.datasets import xxx`来import所需要的类或者函数，后者是`mmcv`库的基本操作，用来统一注册管理某些相似的模块，在这里就是统一管理数据集模块。`capeformer/datasets`下包含两个子文件夹，一个是`datasets`，包括了对数据集的定义，另一个是`pipelines`，包括了对数据集预处理的各个函数。

模型相关代码位于`capeformer/models`文件夹下，其包含一个`py`文件：`__init__.py`，同样也是用于将该文件夹下有用的类和函数都import到当前文件夹下，从而可以直接使用`from capeformer.models import xxx`来import所需要的类或者函数。`capeformer/models`下包含三个子文件夹：`detectors`，`keypoints_head`和`utils`，其内分别定义各类模块。在仔细看它们内部的模块之前，先看一眼configuration文件里对model的configuration的定义。

以`configs/mp100/capeformer/1shot/two_stage_split1_config.py`为例，来介绍各个部分的代码。

**2. 数据集**

`configs/mp100/capeformer/1shot/two_stage_split1_config.py`内与数据集相关的部分分为两类：与数据集定义相关，以及与数据集预处理相关。其中与数据集定义相关的部分定义如下：

```python
channel_cfg = dict(
    num_output_channels=1,
    dataset_joints=1,
    dataset_channel=[[0,],],
    inference_channel=[0,],
    max_kpt_num=100)

data_cfg = dict(
    image_size=[256, 256],
    heatmap_size=[64, 64],
    num_output_channels=channel_cfg['num_output_channels'],
    num_joints=channel_cfg['dataset_joints'],
    dataset_channel=channel_cfg['dataset_channel'],
    inference_channel=channel_cfg['inference_channel'])

data_root = 'data/mp100'
data = dict(
            samples_per_gpu=16,
            workers_per_gpu=8,
            train=dict(
                       type='TransformerPoseDataset',
                       ann_file=f'{data_root}/annotations/mp100_split1_train.json',
                       #img_prefix=f'{data_root}/images/',
                       img_prefix=f'{data_root}',
                       data_cfg=data_cfg,
                       valid_class_ids=None,
                       max_kpt_num=channel_cfg['max_kpt_num'],
                       num_shots=1,
                       pipeline=train_pipeline
                      ),
            val=dict(
                     type='TransformerPoseDataset',
                     ann_file=f'{data_root}/annotations/mp100_split1_val.json',
                     #img_prefix=f'{data_root}/images/',
                     img_prefix=f'{data_root}',
                     data_cfg=data_cfg,
                     valid_class_ids=None,
                     max_kpt_num=channel_cfg['max_kpt_num'],
                     num_shots=1,
                     num_queries=15,
                     num_episodes=100,
                     pipeline=valid_pipeline
                    ),
            test=dict(
                      type='TestPoseDataset',
                      ann_file=f'{data_root}/annotations/mp100_split1_test.json',
                      #img_prefix=f'{data_root}/images/',
                      img_prefix=f'{data_root}',
                      data_cfg=data_cfg,
                      valid_class_ids=None,
                      max_kpt_num=channel_cfg['max_kpt_num'],
                      num_shots=1,
                      num_queries=15,
                      num_episodes=200,  
                      pck_threshold_list=[0.05, 0.10, 0.15, 0.2, 0.25],
                      pipeline=test_pipeline
                     ),
            )
```

与数据集定义相关的代码位于`capeformer/datasets/datasets/mp100`下，有`transformer_dataset.py`，`transformer_base_dataset.py`，`test_dataset.py`和`test_base_dataset.py`四个文件（还有两个文件`fewshot_dataset.py`和`fewshot_base_dataset.py`，因为并没有被使用，所以忽略），它们内部分别定义了`TransformerPoseDataset`，`TransformerBaseDataset`，`TestPoseDataset`和`TestBaseDataset`，其中`TransformerPoseDataset`继承`TransformerBaseDataset`，`TestPoseDataset`继承`TestBaseDataset`。

`TransformerBaseDataset`继承了`torch.utils.data.Dataset`，其实现了`__getitem__`和`__len__`这两个基本方法，其`__getitem__`函数内部从`self.paired_samples`一个个获取数据，而`self.paired_samples`是在`TransformerPoseDataset`内由`random_paired_samples()`或者`make_paired_samples()`返回的，其是一个`list`，每一项也是一个`list`，包含`num_shots + 1`个图片的coco_id。`TransformerBaseDataset`的`__getitem__`最后的`Xq`是一个`dict`，而`Xs`是一个`dict`的`list`，有`num_shots`项。最后由一个`_merge_obj()`函数将`Xs`和`Xq`的相同信息整合，存储为一个`dict`，作为`__getitem__`的输出。在`TransformerPoseDataset`内，其通过`_get_db`函数对原数据进行处理，原数据格式是coco格式，使用的是`xtcocotools.coco.COCO`来对数据进行处理。


与数据集预处理的部分定义如下：

```python
train_pipeline = [
                  dict(type='LoadImageFromFile'),
                  dict(
                       type='TopDownGetRandomScaleRotation', rot_factor=15,
                       scale_factor=0.15),
                  dict(type='TopDownAffineFewShot'),
                  dict(type='ToTensor'),
                  dict(
                       type='NormalizeTensor',
                       mean=[0.485, 0.456, 0.406],
                       std=[0.229, 0.224, 0.225]),
                  dict(type='TopDownGenerateTargetFewShot', sigma=2),
                  dict(
                       type='Collect',
                       keys=['img', 'target', 'target_weight'],
                       meta_keys=[
                                  'image_file', 'joints_3d', 'joints_3d_visible', 'center', 'scale',
                                  'rotation', 'bbox_score', 'flip_pairs', 'category_id'
                                 ]
                      ),
                  ]

valid_pipeline = [
                  dict(type='LoadImageFromFile'),
                  dict(type='TopDownAffineFewShot'),
                  dict(type='ToTensor'),
                  dict(
                       type='NormalizeTensor',
                       mean=[0.485, 0.456, 0.406],
                       std=[0.229, 0.224, 0.225]),
                  dict(type='TopDownGenerateTargetFewShot', sigma=2),
                  dict(
                       type='Collect',
                       keys=['img', 'target', 'target_weight'],
                       meta_keys=[
                                  'image_file', 'center', 'scale', 'rotation', 'bbox_score',
                                  'flip_pairs', 'category_id'
                                 ]
                      ),
                 ]

test_pipeline = valid_pipeline
```


**3. 模型**

`configs/mp100/capeformer/1shot/two_stage_split1_config.py`的`model`是一个字典，表明了模型各个部分的定义和之间的关系，具体如下：

```python
model = dict(
             type='TransformerPoseTwoStage',
             pretrained='torchvision://resnet50',
             encoder_config=dict(
                                 type='ResNet', depth=50, out_indices=(3, )
                                ),
             keypoint_head=dict(
                                type='TwoStageHead',
                                in_channels=2048,
                                transformer=dict(
                                                 type='TwoStageSupportRefineTransformer',
                                                 d_model=256, nhead=8, num_encoder_layers=3,
                                                 num_decoder_layers=3, dim_feedforward=2048,
                                                 dropout=0.1, similarity_proj_dim=256,
                                                 dynamic_proj_dim=128, activation="relu",
                                                 normalize_before=False, return_intermediate_dec=True
                                                ),
                                share_kpt_branch=False, num_decoder_layer=3, with_heatmap_loss=True,
                                heatmap_loss_weight=2.0, support_embedding_type="fixed",
                                num_support=100, support_order_dropout=-1,
                                positional_encoding=dict(
                                                         type='SinePositionalEncoding',
                                                         num_feats=128, normalize=True
                                                        )
                               ),
             share_backbone=True,
             # training and testing settings
             train_cfg=dict(),
             test_cfg=dict(
                           flip_test=False,
                           post_process='default',
                           shift_heatmap=True,
                           modulate_kernel=11
                          )
             )
```

由上述configuration文件可知，模型以`TransformerPoseTwoStage`为主体，里面包括了一个由`ResNet`构成的encoder，用来从输入图片里获取image features，和一个由`TwoStageHead`构成的keypoint_head，用来输出similarity maps和keypoints offset。其中该keypoint_head内部还包含了一个由`TwoStageSupportRefineTransformer`定义的transformer，用来refine由encoder获得的image features，以及一个由`SinePositionalEncoding`构成的positional_encoding，用来给query图片features增加位置信息编码。


### \[**CVPR 2024**\] [ESCAPE: Encoding Super-keypoints for Category-Agnostic Pose Estimation](https://github.com/khoiucd/escape-tgt)

### \[**CVPR 2024**\] [Meta-Point Learning and Refining for Category-Agnostic Pose Estimation](https://github.com/chenbys/MetaPoint)

### \[**ECCV 2024 Highlight**\] [X-Pose : Detecting Any Keypoints](https://yangjie-cv.github.io/X-Pose/)

### \[**ECCV 2024**\] [SCAPE: A Simple and Strong Category-Agnostic Pose Estimator](https://github.com/tiny-smart/SCAPE)

## Graph-based methods

### \[**ECCV 2024**\] [A Graph-Based Approach for Category-Agnostic Pose Estimation](https://orhir.github.io/pose-anything/)

### \[**Arxiv 2025**\] [Edge Weight Prediction For Category-Agnostic Pose Estimation](https://orhir.github.io/edge_cape/)

## Text-based methods

### \[**ICLR 2025**\] [CapeX: Category-Agnostic Pose Estimation from Textual Point Explanation](https://github.com/matanr/capex)
